nohup: 忽略输入
MobileNetV2(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
    (1): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (7): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (8): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (9): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (10): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (11): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (12): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (14): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (16): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (17): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (18): ConvBNReLU(
      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
  )
  (classifier): Sequential(
    (0): Dropout(p=0.2, inplace=False)
    (1): Linear(in_features=1280, out_features=3, bias=True)
  )
)
experiment6.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  batch_xx,labels = Variable(torch.tensor(batch_xx).cuda()), Variable(torch.tensor(labels).cuda())
Batch 1, Train Loss:0.037433, Train ACC:0.3667
Batch 2, Train Loss:0.036454, Train ACC:0.3833
Batch 3, Train Loss:0.037555, Train ACC:0.4333
Batch 4, Train Loss:0.036880, Train ACC:0.4583
Batch 5, Train Loss:0.036511, Train ACC:0.4600
Batch 6, Train Loss:0.035707, Train ACC:0.4778
Batch 7, Train Loss:0.034967, Train ACC:0.4810
Batch 8, Train Loss:0.034338, Train ACC:0.4875
Batch 9, Train Loss:0.033950, Train ACC:0.4815
Batch 10, Train Loss:0.033483, Train ACC:0.4933
Batch 11, Train Loss:0.033204, Train ACC:0.5121
Batch 12, Train Loss:0.032930, Train ACC:0.5222
Batch 13, Train Loss:0.032493, Train ACC:0.5590
Batch 14, Train Loss:0.031935, Train ACC:0.5905
Batch 15, Train Loss:0.031533, Train ACC:0.6178
Batch 16, Train Loss:0.031190, Train ACC:0.6396
Batch 17, Train Loss:0.030760, Train ACC:0.6549
Batch 18, Train Loss:0.030343, Train ACC:0.6704
Batch 19, Train Loss:0.030134, Train ACC:0.6789
Batch 20, Train Loss:0.029647, Train ACC:0.6950
Batch 21, Train Loss:0.029160, Train ACC:0.7095
Batch 22, Train Loss:0.028713, Train ACC:0.7227
Batch 23, Train Loss:0.028315, Train ACC:0.7348
Batch 24, Train Loss:0.027922, Train ACC:0.7458
Batch 25, Train Loss:0.027521, Train ACC:0.7560
Batch 26, Train Loss:0.027099, Train ACC:0.7654
Batch 27, Train Loss:0.026746, Train ACC:0.7741
Batch 28, Train Loss:0.026547, Train ACC:0.7750
Batch 29, Train Loss:0.026111, Train ACC:0.7828
Batch 30, Train Loss:0.025666, Train ACC:0.7900
Batch 31, Train Loss:0.025235, Train ACC:0.7968
Batch 32, Train Loss:0.024937, Train ACC:0.8021
Batch 33, Train Loss:0.024636, Train ACC:0.8081
Batch 34, Train Loss:0.024465, Train ACC:0.8127
Batch 35, Train Loss:0.024091, Train ACC:0.8181
Batch 36, Train Loss:0.023669, Train ACC:0.8231
Batch 37, Train Loss:0.023614, Train ACC:0.8261
Batch 38, Train Loss:0.023320, Train ACC:0.8307
Batch 39, Train Loss:0.023023, Train ACC:0.8350
Batch 40, Train Loss:0.022661, Train ACC:0.8392
Batch 41, Train Loss:0.022448, Train ACC:0.8431
Batch 42, Train Loss:0.022161, Train ACC:0.8468
Batch 43, Train Loss:0.021944, Train ACC:0.8504
Batch 44, Train Loss:0.021594, Train ACC:0.8538
Batch 45, Train Loss:0.021346, Train ACC:0.8570
Batch 46, Train Loss:0.021120, Train ACC:0.8601
Batch 47, Train Loss:0.020861, Train ACC:0.8631
Batch 48, Train Loss:0.020604, Train ACC:0.8660
Batch 49, Train Loss:0.020413, Train ACC:0.8687
Batch 50, Train Loss:0.020125, Train ACC:0.8713
Batch 51, Train Loss:0.019892, Train ACC:0.8739
Batch 52, Train Loss:0.019698, Train ACC:0.8763
Batch 53, Train Loss:0.019452, Train ACC:0.8786
Batch 54, Train Loss:0.019431, Train ACC:0.8778
Batch 55, Train Loss:0.019211, Train ACC:0.8800
Batch 56, Train Loss:0.019002, Train ACC:0.8821
Batch 57, Train Loss:0.018773, Train ACC:0.8842
Batch 58, Train Loss:0.018598, Train ACC:0.8862
Batch 59, Train Loss:0.018497, Train ACC:0.8881
Batch 60, Train Loss:0.018315, Train ACC:0.8900
Batch 61, Train Loss:0.018120, Train ACC:0.8918
Batch 62, Train Loss:0.017952, Train ACC:0.8935
Batch 63, Train Loss:0.017795, Train ACC:0.8952
Batch 64, Train Loss:0.017578, Train ACC:0.8969
Batch 65, Train Loss:0.017491, Train ACC:0.8985
Batch 66, Train Loss:0.017286, Train ACC:0.9000
Batch 67, Train Loss:0.017137, Train ACC:0.9015
Batch 68, Train Loss:0.016943, Train ACC:0.9029
Batch 69, Train Loss:0.016764, Train ACC:0.9043
Batch 70, Train Loss:0.016590, Train ACC:0.9057
Batch 71, Train Loss:0.016412, Train ACC:0.9070
Batch 72, Train Loss:0.016311, Train ACC:0.9083
Batch 73, Train Loss:0.016301, Train ACC:0.9096
Batch 74, Train Loss:0.016136, Train ACC:0.9108
Batch 75, Train Loss:0.015982, Train ACC:0.9120
Batch 76, Train Loss:0.015821, Train ACC:0.9132
Batch 77, Train Loss:0.015663, Train ACC:0.9143
Batch 78, Train Loss:0.015542, Train ACC:0.9154
Batch 79, Train Loss:0.015406, Train ACC:0.9165
Batch 80, Train Loss:0.015248, Train ACC:0.9175
Batch 81, Train Loss:0.015092, Train ACC:0.9185
Batch 82, Train Loss:0.014973, Train ACC:0.9195
Batch 83, Train Loss:0.014880, Train ACC:0.9205
Batch 84, Train Loss:0.014751, Train ACC:0.9214
Batch 85, Train Loss:0.014607, Train ACC:0.9224
Batch 86, Train Loss:0.014463, Train ACC:0.9233
Batch 87, Train Loss:0.014467, Train ACC:0.9241
Batch 88, Train Loss:0.014365, Train ACC:0.9250
Batch 89, Train Loss:0.014278, Train ACC:0.9258
Batch 90, Train Loss:0.014143, Train ACC:0.9267
Batch 91, Train Loss:0.014009, Train ACC:0.9275
Batch 92, Train Loss:0.013874, Train ACC:0.9283
Batch 93, Train Loss:0.013835, Train ACC:0.9290
Batch 94, Train Loss:0.013732, Train ACC:0.9298
Batch 95, Train Loss:0.013609, Train ACC:0.9305
Batch 96, Train Loss:0.013490, Train ACC:0.9313
Batch 97, Train Loss:0.013377, Train ACC:0.9320
Batch 98, Train Loss:0.013268, Train ACC:0.9327
Batch 99, Train Loss:0.013164, Train ACC:0.9333
Batch 100, Train Loss:0.013066, Train ACC:0.9340
Batch 101, Train Loss:0.012956, Train ACC:0.9347
Batch 102, Train Loss:0.012868, Train ACC:0.9353
Batch 103, Train Loss:0.012831, Train ACC:0.9359
Batch 104, Train Loss:0.012727, Train ACC:0.9365
Batch 105, Train Loss:0.012745, Train ACC:0.9356
Batch 106, Train Loss:0.012639, Train ACC:0.9362
Batch 107, Train Loss:0.012543, Train ACC:0.9368
Batch 108, Train Loss:0.012450, Train ACC:0.9373
Batch 109, Train Loss:0.012395, Train ACC:0.9379
Batch 110, Train Loss:0.012295, Train ACC:0.9385
Batch 111, Train Loss:0.012201, Train ACC:0.9390
Batch 112, Train Loss:0.012147, Train ACC:0.9396
Batch 113, Train Loss:0.012071, Train ACC:0.9401
Batch 114, Train Loss:0.012033, Train ACC:0.9406
Batch 115, Train Loss:0.011982, Train ACC:0.9412
Batch 116, Train Loss:0.011923, Train ACC:0.9417
Batch 117, Train Loss:0.011881, Train ACC:0.9422
Batch 118, Train Loss:0.011795, Train ACC:0.9427
Batch 119, Train Loss:0.011720, Train ACC:0.9431
Batch 120, Train Loss:0.011633, Train ACC:0.9436
Batch 121, Train Loss:0.011549, Train ACC:0.9441
Batch 122, Train Loss:0.011464, Train ACC:0.9445
Batch 123, Train Loss:0.011396, Train ACC:0.9450
Batch 124, Train Loss:0.011326, Train ACC:0.9454
Batch 125, Train Loss:0.011257, Train ACC:0.9459
Batch 126, Train Loss:0.011185, Train ACC:0.9463
Batch 127, Train Loss:0.011123, Train ACC:0.9467
Batch 128, Train Loss:0.011057, Train ACC:0.9471
Batch 129, Train Loss:0.010985, Train ACC:0.9475
Batch 130, Train Loss:0.010911, Train ACC:0.9479
Batch 131, Train Loss:0.010843, Train ACC:0.9483
Batch 132, Train Loss:0.010768, Train ACC:0.9487
Batch 133, Train Loss:0.010710, Train ACC:0.9491
Batch 134, Train Loss:0.010640, Train ACC:0.9495
Batch 135, Train Loss:0.010571, Train ACC:0.9499
Batch 136, Train Loss:0.010503, Train ACC:0.9502
Batch 137, Train Loss:0.010461, Train ACC:0.9506
Batch 138, Train Loss:0.010410, Train ACC:0.9510
Batch 139, Train Loss:0.010344, Train ACC:0.9513
Batch 140, Train Loss:0.010281, Train ACC:0.9517
Batch 141, Train Loss:0.010217, Train ACC:0.9520
Batch 142, Train Loss:0.010156, Train ACC:0.9523
Batch 143, Train Loss:0.010097, Train ACC:0.9527
Batch 144, Train Loss:0.010034, Train ACC:0.9530
Batch 145, Train Loss:0.009971, Train ACC:0.9533
Batch 146, Train Loss:0.009909, Train ACC:0.9537
Batch 147, Train Loss:0.009856, Train ACC:0.9540
Batch 148, Train Loss:0.009798, Train ACC:0.9543
Batch 149, Train Loss:0.009737, Train ACC:0.9546
Batch 150, Train Loss:0.009676, Train ACC:0.9549
Batch 151, Train Loss:0.009627, Train ACC:0.9552
Batch 152, Train Loss:0.009569, Train ACC:0.9555
Batch 153, Train Loss:0.009518, Train ACC:0.9558
Batch 154, Train Loss:0.009463, Train ACC:0.9561
Batch 155, Train Loss:0.009406, Train ACC:0.9563
Batch 156, Train Loss:0.009359, Train ACC:0.9566
Batch 157, Train Loss:0.009304, Train ACC:0.9569
Batch 158, Train Loss:0.009249, Train ACC:0.9572
Batch 159, Train Loss:0.009197, Train ACC:0.9574
Batch 160, Train Loss:0.009144, Train ACC:0.9577
Batch 161, Train Loss:0.009098, Train ACC:0.9580
Batch 162, Train Loss:0.009063, Train ACC:0.9582
Batch 163, Train Loss:0.009013, Train ACC:0.9585
Batch 164, Train Loss:0.008965, Train ACC:0.9587
Batch 165, Train Loss:0.008920, Train ACC:0.9590
Batch 166, Train Loss:0.008871, Train ACC:0.9592
Batch 167, Train Loss:0.008823, Train ACC:0.9595
Batch 168, Train Loss:0.008785, Train ACC:0.9597
Batch 169, Train Loss:0.008738, Train ACC:0.9600
Batch 170, Train Loss:0.008698, Train ACC:0.9602
Batch 171, Train Loss:0.008657, Train ACC:0.9604
Batch 172, Train Loss:0.008622, Train ACC:0.9607
Batch 173, Train Loss:0.008577, Train ACC:0.9609
Batch 174, Train Loss:0.008532, Train ACC:0.9611
Batch 175, Train Loss:0.008488, Train ACC:0.9613
Batch 176, Train Loss:0.008450, Train ACC:0.9616
Batch 177, Train Loss:0.008410, Train ACC:0.9618
Batch 178, Train Loss:0.008367, Train ACC:0.9620
Batch 179, Train Loss:0.008325, Train ACC:0.9622
Batch 180, Train Loss:0.008281, Train ACC:0.9624
Batch 181, Train Loss:0.008238, Train ACC:0.9626
Batch 182, Train Loss:0.008197, Train ACC:0.9628
Batch 183, Train Loss:0.008155, Train ACC:0.9630
Batch 184, Train Loss:0.008121, Train ACC:0.9632
Batch 185, Train Loss:0.008097, Train ACC:0.9634
Batch 186, Train Loss:0.008055, Train ACC:0.9636
Batch 187, Train Loss:0.008042, Train ACC:0.9638
Batch 188, Train Loss:0.008001, Train ACC:0.9640
Batch 189, Train Loss:0.007974, Train ACC:0.9642
Batch 190, Train Loss:0.007940, Train ACC:0.9644
Batch 191, Train Loss:0.007903, Train ACC:0.9646
Batch 192, Train Loss:0.007868, Train ACC:0.9648
Batch 193, Train Loss:0.007831, Train ACC:0.9649
Batch 194, Train Loss:0.007794, Train ACC:0.9651
Batch 195, Train Loss:0.007759, Train ACC:0.9653
Batch 196, Train Loss:0.007723, Train ACC:0.9655
Batch 197, Train Loss:0.007691, Train ACC:0.9657
Batch 198, Train Loss:0.007656, Train ACC:0.9658
Batch 199, Train Loss:0.007627, Train ACC:0.9660
Batch 200, Train Loss:0.007592, Train ACC:0.9662
Batch 201, Train Loss:0.007567, Train ACC:0.9663
Batch 202, Train Loss:0.007532, Train ACC:0.9665
Batch 203, Train Loss:0.007499, Train ACC:0.9667
Batch 204, Train Loss:0.007465, Train ACC:0.9668
Batch 205, Train Loss:0.007431, Train ACC:0.9670
Batch 206, Train Loss:0.007409, Train ACC:0.9672
Batch 207, Train Loss:0.007376, Train ACC:0.9673
Batch 208, Train Loss:0.007347, Train ACC:0.9675
Batch 209, Train Loss:0.007314, Train ACC:0.9676
Batch 210, Train Loss:0.007297, Train ACC:0.9678
Batch 211, Train Loss:0.007265, Train ACC:0.9679
Batch 212, Train Loss:0.007241, Train ACC:0.9681
Batch 213, Train Loss:0.007209, Train ACC:0.9682
Batch 214, Train Loss:0.007179, Train ACC:0.9684
Batch 215, Train Loss:0.007151, Train ACC:0.9685
Batch 216, Train Loss:0.007126, Train ACC:0.9687
Batch 217, Train Loss:0.007096, Train ACC:0.9688
Batch 218, Train Loss:0.007067, Train ACC:0.9690
Batch 219, Train Loss:0.007041, Train ACC:0.9691
Batch 220, Train Loss:0.007020, Train ACC:0.9692
Batch 221, Train Loss:0.006993, Train ACC:0.9694
Batch 222, Train Loss:0.006963, Train ACC:0.9695
Batch 223, Train Loss:0.006972, Train ACC:0.9697
Batch 224, Train Loss:0.006956, Train ACC:0.9698
Batch 225, Train Loss:0.006927, Train ACC:0.9699
Batch 226, Train Loss:0.006900, Train ACC:0.9701
Batch 227, Train Loss:0.006883, Train ACC:0.9702
Batch 228, Train Loss:0.006856, Train ACC:0.9703
Batch 229, Train Loss:0.006828, Train ACC:0.9705
Batch 230, Train Loss:0.006800, Train ACC:0.9706
Batch 231, Train Loss:0.006773, Train ACC:0.9707
Batch 232, Train Loss:0.006747, Train ACC:0.9708
Batch 233, Train Loss:0.006720, Train ACC:0.9710
Batch 234, Train Loss:0.006693, Train ACC:0.9711
Batch 235, Train Loss:0.006672, Train ACC:0.9712
Batch 236, Train Loss:0.006646, Train ACC:0.9713
Batch 237, Train Loss:0.006620, Train ACC:0.9714
Batch 238, Train Loss:0.006596, Train ACC:0.9716
Batch 239, Train Loss:0.006572, Train ACC:0.9717
Batch 240, Train Loss:0.006565, Train ACC:0.9718
Batch 241, Train Loss:0.006548, Train ACC:0.9719
Batch 242, Train Loss:0.006523, Train ACC:0.9720
Batch 243, Train Loss:0.006498, Train ACC:0.9722
Batch 244, Train Loss:0.006475, Train ACC:0.9723
Batch 245, Train Loss:0.006451, Train ACC:0.9724
Batch 246, Train Loss:0.006427, Train ACC:0.9725
Batch 247, Train Loss:0.006403, Train ACC:0.9726
Batch 248, Train Loss:0.006378, Train ACC:0.9727
Batch 249, Train Loss:0.006355, Train ACC:0.9728
Batch 250, Train Loss:0.006334, Train ACC:0.9729
Batch 251, Train Loss:0.006311, Train ACC:0.9730
Batch 252, Train Loss:0.006287, Train ACC:0.9731
Batch 253, Train Loss:0.006265, Train ACC:0.9733
Batch 254, Train Loss:0.006244, Train ACC:0.9734
Batch 255, Train Loss:0.006221, Train ACC:0.9735
Batch 256, Train Loss:0.006202, Train ACC:0.9736
Batch 257, Train Loss:0.006179, Train ACC:0.9737
Batch 258, Train Loss:0.006160, Train ACC:0.9738
Batch 259, Train Loss:0.006144, Train ACC:0.9739
Batch 260, Train Loss:0.006122, Train ACC:0.9740
Batch 261, Train Loss:0.006100, Train ACC:0.9741
Batch 262, Train Loss:0.006078, Train ACC:0.9742
Batch 263, Train Loss:0.006056, Train ACC:0.9743
Batch 264, Train Loss:0.006035, Train ACC:0.9744
Batch 265, Train Loss:0.006015, Train ACC:0.9745
Batch 266, Train Loss:0.006005, Train ACC:0.9746
Batch 267, Train Loss:0.005984, Train ACC:0.9747
Batch 268, Train Loss:0.005963, Train ACC:0.9748
Batch 269, Train Loss:0.005942, Train ACC:0.9748
Batch 270, Train Loss:0.005923, Train ACC:0.9749
Batch 271, Train Loss:0.005902, Train ACC:0.9750
Batch 272, Train Loss:0.005887, Train ACC:0.9751
Batch 273, Train Loss:0.005868, Train ACC:0.9752
Batch 274, Train Loss:0.005847, Train ACC:0.9753
Batch 275, Train Loss:0.005827, Train ACC:0.9754
Batch 276, Train Loss:0.005807, Train ACC:0.9755
Batch 277, Train Loss:0.005790, Train ACC:0.9756
Batch 278, Train Loss:0.005771, Train ACC:0.9757
Batch 279, Train Loss:0.005754, Train ACC:0.9757
Batch 280, Train Loss:0.005734, Train ACC:0.9758
Batch 281, Train Loss:0.005715, Train ACC:0.9759
Batch 282, Train Loss:0.005697, Train ACC:0.9760
Batch 283, Train Loss:0.005678, Train ACC:0.9761
Batch 284, Train Loss:0.005660, Train ACC:0.9762
Batch 285, Train Loss:0.005642, Train ACC:0.9763
Batch 286, Train Loss:0.005624, Train ACC:0.9763
Batch 287, Train Loss:0.005605, Train ACC:0.9764
Batch 288, Train Loss:0.005587, Train ACC:0.9765
Batch 289, Train Loss:0.005568, Train ACC:0.9766
Batch 290, Train Loss:0.005550, Train ACC:0.9767
Batch 291, Train Loss:0.005531, Train ACC:0.9767
Batch 292, Train Loss:0.005514, Train ACC:0.9768
Batch 293, Train Loss:0.005496, Train ACC:0.9769
Batch 294, Train Loss:0.005494, Train ACC:0.9770
Batch 295, Train Loss:0.005476, Train ACC:0.9771
Batch 296, Train Loss:0.005458, Train ACC:0.9771
Batch 297, Train Loss:0.005441, Train ACC:0.9772
Batch 298, Train Loss:0.005423, Train ACC:0.9773
Batch 299, Train Loss:0.005406, Train ACC:0.9774
Batch 300, Train Loss:0.005389, Train ACC:0.9774
预测标签：tensor([2, 1, 2, 0, 1, 1, 0, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 1, 2, 0, 1, 2, 2,
        1, 1, 2, 2, 2, 2], device='cuda:0'), 真实标签：tensor([2, 1, 2, 0, 1, 1, 0, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 1, 2, 0, 1, 2, 2,
        1, 1, 2, 2, 2, 2], device='cuda:0')
ResNet(
  (conv1): Sequential(
    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  )
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (6): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (7): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (8): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (9): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (10): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (11): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (12): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (13): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (14): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (15): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (16): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (17): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (18): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (19): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (20): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (21): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (22): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Sequential(
    (0): Linear(in_features=2048, out_features=3, bias=True)
  )
)
Batch 1, Train Loss:0.036436, Train ACC:0.3333
Batch 2, Train Loss:0.046071, Train ACC:0.3333
Batch 3, Train Loss:0.043187, Train ACC:0.3000
Batch 4, Train Loss:0.039834, Train ACC:0.3333
Batch 5, Train Loss:0.040221, Train ACC:0.3333
Batch 6, Train Loss:0.038580, Train ACC:0.3500
Batch 7, Train Loss:0.037371, Train ACC:0.3524
Batch 8, Train Loss:0.036384, Train ACC:0.3958
Batch 9, Train Loss:0.036468, Train ACC:0.4185
Batch 10, Train Loss:0.034908, Train ACC:0.4533
Batch 11, Train Loss:0.034856, Train ACC:0.4333
Batch 12, Train Loss:0.034041, Train ACC:0.4500
Batch 13, Train Loss:0.033009, Train ACC:0.4923
Batch 14, Train Loss:0.032210, Train ACC:0.4881
Batch 15, Train Loss:0.031905, Train ACC:0.4711
Batch 16, Train Loss:0.031290, Train ACC:0.4833
Batch 17, Train Loss:0.030809, Train ACC:0.4725
Batch 18, Train Loss:0.030162, Train ACC:0.5019
Batch 19, Train Loss:0.029359, Train ACC:0.5105
Batch 20, Train Loss:0.028737, Train ACC:0.5350
Batch 21, Train Loss:0.028440, Train ACC:0.5571
Batch 22, Train Loss:0.028056, Train ACC:0.5773
Batch 23, Train Loss:0.027877, Train ACC:0.5710
Batch 24, Train Loss:0.027311, Train ACC:0.5889
Batch 25, Train Loss:0.026804, Train ACC:0.6053
Batch 26, Train Loss:0.026326, Train ACC:0.6205
Batch 27, Train Loss:0.026076, Train ACC:0.6346
Batch 28, Train Loss:0.025676, Train ACC:0.6476
Batch 29, Train Loss:0.025320, Train ACC:0.6598
Batch 30, Train Loss:0.024937, Train ACC:0.6711
Batch 31, Train Loss:0.024619, Train ACC:0.6817
Batch 32, Train Loss:0.024167, Train ACC:0.6917
Batch 33, Train Loss:0.023796, Train ACC:0.7010
Batch 34, Train Loss:0.023373, Train ACC:0.7098
Batch 35, Train Loss:0.022948, Train ACC:0.7181
Batch 36, Train Loss:0.022617, Train ACC:0.7259
Batch 37, Train Loss:0.022263, Train ACC:0.7333
Batch 38, Train Loss:0.021863, Train ACC:0.7404
Batch 39, Train Loss:0.021627, Train ACC:0.7470
Batch 40, Train Loss:0.021302, Train ACC:0.7533
Batch 41, Train Loss:0.021139, Train ACC:0.7593
Batch 42, Train Loss:0.020855, Train ACC:0.7651
Batch 43, Train Loss:0.020677, Train ACC:0.7705
Batch 44, Train Loss:0.020465, Train ACC:0.7758
Batch 45, Train Loss:0.020177, Train ACC:0.7807
Batch 46, Train Loss:0.020088, Train ACC:0.7855
Batch 47, Train Loss:0.019848, Train ACC:0.7901
Batch 48, Train Loss:0.019617, Train ACC:0.7944
Batch 49, Train Loss:0.019446, Train ACC:0.7986
Batch 50, Train Loss:0.019336, Train ACC:0.7993
Batch 51, Train Loss:0.019174, Train ACC:0.7987
Batch 52, Train Loss:0.018994, Train ACC:0.7994
Batch 53, Train Loss:0.018771, Train ACC:0.8031
Batch 54, Train Loss:0.018589, Train ACC:0.8068
Batch 55, Train Loss:0.018416, Train ACC:0.8103
Batch 56, Train Loss:0.018280, Train ACC:0.8137
Batch 57, Train Loss:0.018090, Train ACC:0.8170
Batch 58, Train Loss:0.017918, Train ACC:0.8201
Batch 59, Train Loss:0.017773, Train ACC:0.8232
Batch 60, Train Loss:0.017562, Train ACC:0.8261
Batch 61, Train Loss:0.017470, Train ACC:0.8290
Batch 62, Train Loss:0.017278, Train ACC:0.8317
Batch 63, Train Loss:0.017130, Train ACC:0.8344
Batch 64, Train Loss:0.016966, Train ACC:0.8370
Batch 65, Train Loss:0.016938, Train ACC:0.8395
Batch 66, Train Loss:0.016790, Train ACC:0.8419
Batch 67, Train Loss:0.016659, Train ACC:0.8443
Batch 68, Train Loss:0.016558, Train ACC:0.8466
Batch 69, Train Loss:0.016395, Train ACC:0.8488
Batch 70, Train Loss:0.016242, Train ACC:0.8510
Batch 71, Train Loss:0.016123, Train ACC:0.8531
Batch 72, Train Loss:0.016073, Train ACC:0.8495
Batch 73, Train Loss:0.015966, Train ACC:0.8516
Batch 74, Train Loss:0.015823, Train ACC:0.8536
Batch 75, Train Loss:0.015709, Train ACC:0.8556
Batch 76, Train Loss:0.015593, Train ACC:0.8575
Batch 77, Train Loss:0.015483, Train ACC:0.8567
Batch 78, Train Loss:0.015353, Train ACC:0.8585
Batch 79, Train Loss:0.015206, Train ACC:0.8603
Batch 80, Train Loss:0.015052, Train ACC:0.8621
Batch 81, Train Loss:0.014891, Train ACC:0.8638
Batch 82, Train Loss:0.014892, Train ACC:0.8610
Batch 83, Train Loss:0.014778, Train ACC:0.8627
Batch 84, Train Loss:0.014631, Train ACC:0.8643
Batch 85, Train Loss:0.014523, Train ACC:0.8659
Batch 86, Train Loss:0.014489, Train ACC:0.8674
Batch 87, Train Loss:0.014417, Train ACC:0.8690
Batch 88, Train Loss:0.014384, Train ACC:0.8667
Batch 89, Train Loss:0.014317, Train ACC:0.8682
Batch 90, Train Loss:0.014237, Train ACC:0.8696
Batch 91, Train Loss:0.014164, Train ACC:0.8685
Batch 92, Train Loss:0.014222, Train ACC:0.8641
Batch 93, Train Loss:0.014120, Train ACC:0.8656
Batch 94, Train Loss:0.014016, Train ACC:0.8670
Batch 95, Train Loss:0.013895, Train ACC:0.8684
Batch 96, Train Loss:0.013814, Train ACC:0.8698
Batch 97, Train Loss:0.013734, Train ACC:0.8711
Batch 98, Train Loss:0.013644, Train ACC:0.8724
Batch 99, Train Loss:0.013570, Train ACC:0.8737
Batch 100, Train Loss:0.013480, Train ACC:0.8750
Batch 101, Train Loss:0.013429, Train ACC:0.8746
Batch 102, Train Loss:0.013320, Train ACC:0.8758
Batch 103, Train Loss:0.013235, Train ACC:0.8770
Batch 104, Train Loss:0.013201, Train ACC:0.8782
Batch 105, Train Loss:0.013099, Train ACC:0.8794
Batch 106, Train Loss:0.012996, Train ACC:0.8805
Batch 107, Train Loss:0.012893, Train ACC:0.8816
Batch 108, Train Loss:0.012796, Train ACC:0.8827
Batch 109, Train Loss:0.012877, Train ACC:0.8786
Batch 110, Train Loss:0.012785, Train ACC:0.8797
Batch 111, Train Loss:0.012739, Train ACC:0.8790
Batch 112, Train Loss:0.012699, Train ACC:0.8783
Batch 113, Train Loss:0.012669, Train ACC:0.8767
Batch 114, Train Loss:0.012640, Train ACC:0.8757
Batch 115, Train Loss:0.012573, Train ACC:0.8768
Batch 116, Train Loss:0.012506, Train ACC:0.8779
Batch 117, Train Loss:0.012453, Train ACC:0.8789
Batch 118, Train Loss:0.012412, Train ACC:0.8799
Batch 119, Train Loss:0.012332, Train ACC:0.8810
Batch 120, Train Loss:0.012278, Train ACC:0.8819
Batch 121, Train Loss:0.012223, Train ACC:0.8829
Batch 122, Train Loss:0.012166, Train ACC:0.8839
Batch 123, Train Loss:0.012095, Train ACC:0.8848
Batch 124, Train Loss:0.012062, Train ACC:0.8847
Batch 125, Train Loss:0.012070, Train ACC:0.8832
Batch 126, Train Loss:0.012001, Train ACC:0.8841
Batch 127, Train Loss:0.011929, Train ACC:0.8850
Batch 128, Train Loss:0.011864, Train ACC:0.8859
Batch 129, Train Loss:0.011797, Train ACC:0.8868
Batch 130, Train Loss:0.011888, Train ACC:0.8836
Batch 131, Train Loss:0.011874, Train ACC:0.8845
Batch 132, Train Loss:0.011807, Train ACC:0.8854
Batch 133, Train Loss:0.011805, Train ACC:0.8842
Batch 134, Train Loss:0.011786, Train ACC:0.8823
Batch 135, Train Loss:0.011760, Train ACC:0.8832
Batch 136, Train Loss:0.011708, Train ACC:0.8841
Batch 137, Train Loss:0.011648, Train ACC:0.8849
Batch 138, Train Loss:0.011590, Train ACC:0.8857
Batch 139, Train Loss:0.011520, Train ACC:0.8866
Batch 140, Train Loss:0.011467, Train ACC:0.8874
Batch 141, Train Loss:0.011405, Train ACC:0.8882
Batch 142, Train Loss:0.011343, Train ACC:0.8890
Batch 143, Train Loss:0.011278, Train ACC:0.8897
Batch 144, Train Loss:0.011238, Train ACC:0.8905
Batch 145, Train Loss:0.011185, Train ACC:0.8913
Batch 146, Train Loss:0.011168, Train ACC:0.8920
Batch 147, Train Loss:0.011110, Train ACC:0.8927
Batch 148, Train Loss:0.011049, Train ACC:0.8935
Batch 149, Train Loss:0.010984, Train ACC:0.8942
Batch 150, Train Loss:0.010924, Train ACC:0.8949
Batch 151, Train Loss:0.010860, Train ACC:0.8956
Batch 152, Train Loss:0.010797, Train ACC:0.8963
Batch 153, Train Loss:0.010741, Train ACC:0.8969
Batch 154, Train Loss:0.010686, Train ACC:0.8976
Batch 155, Train Loss:0.010658, Train ACC:0.8983
Batch 156, Train Loss:0.010601, Train ACC:0.8989
Batch 157, Train Loss:0.010545, Train ACC:0.8996
Batch 158, Train Loss:0.010491, Train ACC:0.9002
Batch 159, Train Loss:0.010446, Train ACC:0.9008
Batch 160, Train Loss:0.010392, Train ACC:0.9015
Batch 161, Train Loss:0.010359, Train ACC:0.9021
Batch 162, Train Loss:0.010313, Train ACC:0.9027
Batch 163, Train Loss:0.010263, Train ACC:0.9033
Batch 164, Train Loss:0.010211, Train ACC:0.9039
Batch 165, Train Loss:0.010159, Train ACC:0.9044
Batch 166, Train Loss:0.010109, Train ACC:0.9050
Batch 167, Train Loss:0.010066, Train ACC:0.9056
Batch 168, Train Loss:0.010020, Train ACC:0.9062
Batch 169, Train Loss:0.009967, Train ACC:0.9067
Batch 170, Train Loss:0.009921, Train ACC:0.9073
Batch 171, Train Loss:0.009872, Train ACC:0.9078
Batch 172, Train Loss:0.009822, Train ACC:0.9083
Batch 173, Train Loss:0.009773, Train ACC:0.9089
Batch 174, Train Loss:0.009722, Train ACC:0.9094
Batch 175, Train Loss:0.009673, Train ACC:0.9099
Batch 176, Train Loss:0.009623, Train ACC:0.9104
Batch 177, Train Loss:0.009581, Train ACC:0.9109
Batch 178, Train Loss:0.009534, Train ACC:0.9114
Batch 179, Train Loss:0.009487, Train ACC:0.9119
Batch 180, Train Loss:0.009441, Train ACC:0.9124
Batch 181, Train Loss:0.009398, Train ACC:0.9129
Batch 182, Train Loss:0.009352, Train ACC:0.9134
Batch 183, Train Loss:0.009346, Train ACC:0.9138
Batch 184, Train Loss:0.009301, Train ACC:0.9143
Batch 185, Train Loss:0.009270, Train ACC:0.9148
Batch 186, Train Loss:0.009263, Train ACC:0.9145
Batch 187, Train Loss:0.009261, Train ACC:0.9150
Batch 188, Train Loss:0.009221, Train ACC:0.9154
Batch 189, Train Loss:0.009181, Train ACC:0.9159
Batch 190, Train Loss:0.009140, Train ACC:0.9163
Batch 191, Train Loss:0.009100, Train ACC:0.9168
Batch 192, Train Loss:0.009065, Train ACC:0.9172
Batch 193, Train Loss:0.009030, Train ACC:0.9176
Batch 194, Train Loss:0.009047, Train ACC:0.9180
Batch 195, Train Loss:0.009020, Train ACC:0.9185
Batch 196, Train Loss:0.008993, Train ACC:0.9189
Batch 197, Train Loss:0.008956, Train ACC:0.9193
Batch 198, Train Loss:0.008921, Train ACC:0.9197
Batch 199, Train Loss:0.008886, Train ACC:0.9201
Batch 200, Train Loss:0.008854, Train ACC:0.9205
Batch 201, Train Loss:0.008830, Train ACC:0.9209
Batch 202, Train Loss:0.008798, Train ACC:0.9213
Batch 203, Train Loss:0.008767, Train ACC:0.9217
Batch 204, Train Loss:0.008734, Train ACC:0.9221
Batch 205, Train Loss:0.008695, Train ACC:0.9224
Batch 206, Train Loss:0.008676, Train ACC:0.9228
Batch 207, Train Loss:0.008640, Train ACC:0.9232
Batch 208, Train Loss:0.008613, Train ACC:0.9236
Batch 209, Train Loss:0.008585, Train ACC:0.9239
Batch 210, Train Loss:0.008550, Train ACC:0.9243
Batch 211, Train Loss:0.008538, Train ACC:0.9246
Batch 212, Train Loss:0.008504, Train ACC:0.9250
Batch 213, Train Loss:0.008475, Train ACC:0.9254
Batch 214, Train Loss:0.008456, Train ACC:0.9257
Batch 215, Train Loss:0.008434, Train ACC:0.9260
Batch 216, Train Loss:0.008413, Train ACC:0.9264
Batch 217, Train Loss:0.008380, Train ACC:0.9267
Batch 218, Train Loss:0.008347, Train ACC:0.9271
Batch 219, Train Loss:0.008314, Train ACC:0.9274
Batch 220, Train Loss:0.008314, Train ACC:0.9277
Batch 221, Train Loss:0.008301, Train ACC:0.9281
Batch 222, Train Loss:0.008269, Train ACC:0.9284
Batch 223, Train Loss:0.008236, Train ACC:0.9287
Batch 224, Train Loss:0.008208, Train ACC:0.9290
Batch 225, Train Loss:0.008185, Train ACC:0.9293
Batch 226, Train Loss:0.008182, Train ACC:0.9289
Batch 227, Train Loss:0.008155, Train ACC:0.9292
Batch 228, Train Loss:0.008142, Train ACC:0.9295
Batch 229, Train Loss:0.008111, Train ACC:0.9298
Batch 230, Train Loss:0.008083, Train ACC:0.9301
Batch 231, Train Loss:0.008052, Train ACC:0.9304
Batch 232, Train Loss:0.008045, Train ACC:0.9307
Batch 233, Train Loss:0.008015, Train ACC:0.9310
Batch 234, Train Loss:0.007986, Train ACC:0.9313
Batch 235, Train Loss:0.007961, Train ACC:0.9316
Batch 236, Train Loss:0.007936, Train ACC:0.9319
Batch 237, Train Loss:0.007913, Train ACC:0.9322
Batch 238, Train Loss:0.007884, Train ACC:0.9325
Batch 239, Train Loss:0.007856, Train ACC:0.9328
Batch 240, Train Loss:0.007827, Train ACC:0.9331
Batch 241, Train Loss:0.007805, Train ACC:0.9333
Batch 242, Train Loss:0.007779, Train ACC:0.9336
Batch 243, Train Loss:0.007750, Train ACC:0.9339
Batch 244, Train Loss:0.007720, Train ACC:0.9342
Batch 245, Train Loss:0.007699, Train ACC:0.9344
Batch 246, Train Loss:0.007673, Train ACC:0.9347
Batch 247, Train Loss:0.007655, Train ACC:0.9350
Batch 248, Train Loss:0.007629, Train ACC:0.9352
Batch 249, Train Loss:0.007602, Train ACC:0.9355
Batch 250, Train Loss:0.007577, Train ACC:0.9357
Batch 251, Train Loss:0.007551, Train ACC:0.9360
Batch 252, Train Loss:0.007524, Train ACC:0.9362
Batch 253, Train Loss:0.007499, Train ACC:0.9365
Batch 254, Train Loss:0.007483, Train ACC:0.9367
Batch 255, Train Loss:0.007458, Train ACC:0.9370
Batch 256, Train Loss:0.007432, Train ACC:0.9372
Batch 257, Train Loss:0.007406, Train ACC:0.9375
Batch 258, Train Loss:0.007381, Train ACC:0.9377
Batch 259, Train Loss:0.007355, Train ACC:0.9380
Batch 260, Train Loss:0.007333, Train ACC:0.9382
Batch 261, Train Loss:0.007314, Train ACC:0.9384
Batch 262, Train Loss:0.007290, Train ACC:0.9387
Batch 263, Train Loss:0.007268, Train ACC:0.9389
Batch 264, Train Loss:0.007251, Train ACC:0.9391
Batch 265, Train Loss:0.007227, Train ACC:0.9394
Batch 266, Train Loss:0.007203, Train ACC:0.9396
Batch 267, Train Loss:0.007184, Train ACC:0.9398
Batch 268, Train Loss:0.007177, Train ACC:0.9400
Batch 269, Train Loss:0.007153, Train ACC:0.9403
Batch 270, Train Loss:0.007130, Train ACC:0.9405
Batch 271, Train Loss:0.007108, Train ACC:0.9407
Batch 272, Train Loss:0.007087, Train ACC:0.9409
Batch 273, Train Loss:0.007067, Train ACC:0.9411
Batch 274, Train Loss:0.007047, Train ACC:0.9414
Batch 275, Train Loss:0.007026, Train ACC:0.9416
Batch 276, Train Loss:0.007004, Train ACC:0.9418
Batch 277, Train Loss:0.006986, Train ACC:0.9420
Batch 278, Train Loss:0.006964, Train ACC:0.9422
Batch 279, Train Loss:0.006941, Train ACC:0.9424
Batch 280, Train Loss:0.006918, Train ACC:0.9426
Batch 281, Train Loss:0.006899, Train ACC:0.9428
Batch 282, Train Loss:0.006887, Train ACC:0.9430
Batch 283, Train Loss:0.006883, Train ACC:0.9432
Batch 284, Train Loss:0.006860, Train ACC:0.9434
Batch 285, Train Loss:0.006838, Train ACC:0.9436
Batch 286, Train Loss:0.006821, Train ACC:0.9438
Batch 287, Train Loss:0.006801, Train ACC:0.9440
Batch 288, Train Loss:0.006783, Train ACC:0.9442
Batch 289, Train Loss:0.006764, Train ACC:0.9444
Batch 290, Train Loss:0.006743, Train ACC:0.9446
Batch 291, Train Loss:0.006723, Train ACC:0.9448
Batch 292, Train Loss:0.006709, Train ACC:0.9450
Batch 293, Train Loss:0.006691, Train ACC:0.9452
Batch 294, Train Loss:0.006672, Train ACC:0.9454
Batch 295, Train Loss:0.006653, Train ACC:0.9455
Batch 296, Train Loss:0.006634, Train ACC:0.9457
Batch 297, Train Loss:0.006614, Train ACC:0.9459
Batch 298, Train Loss:0.006595, Train ACC:0.9461
Batch 299, Train Loss:0.006575, Train ACC:0.9463
Batch 300, Train Loss:0.006558, Train ACC:0.9464
预测标签：tensor([1, 2, 2, 0, 0, 0, 0, 2, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 2, 2, 0, 1, 0, 1,
        2, 2, 2, 2, 0, 1], device='cuda:0'), 真实标签：tensor([1, 2, 2, 0, 0, 0, 0, 2, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 2, 2, 0, 1, 0, 1,
        2, 2, 2, 2, 0, 1], device='cuda:0')
ShuffleNetV2(
  (conv1): Sequential(
    (0): Conv2d(1, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (stage2): Sequential(
    (0): InvertedResidual(
      (branch1): Sequential(
        (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (4): ReLU(inplace=True)
      )
      (branch2): Sequential(
        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (7): ReLU(inplace=True)
      )
    )
    (1): InvertedResidual(
      (branch1): Sequential()
      (branch2): Sequential(
        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)
        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (7): ReLU(inplace=True)
      )
    )
    (2): InvertedResidual(
      (branch1): Sequential()
      (branch2): Sequential(
        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)
        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (7): ReLU(inplace=True)
      )
    )
    (3): InvertedResidual(
      (branch1): Sequential()
      (branch2): Sequential(
        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)
        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (7): ReLU(inplace=True)
      )
    )
  )
  (stage3): Sequential(
    (0): InvertedResidual(
      (branch1): Sequential(
        (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (4): ReLU(inplace=True)
      )
      (branch2): Sequential(
        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (7): ReLU(inplace=True)
      )
    )
    (1): InvertedResidual(
      (branch1): Sequential()
      (branch2): Sequential(
        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (7): ReLU(inplace=True)
      )
    )
    (2): InvertedResidual(
      (branch1): Sequential()
      (branch2): Sequential(
        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (7): ReLU(inplace=True)
      )
    )
    (3): InvertedResidual(
      (branch1): Sequential()
      (branch2): Sequential(
        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (7): ReLU(inplace=True)
      )
    )
    (4): InvertedResidual(
      (branch1): Sequential()
      (branch2): Sequential(
        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (7): ReLU(inplace=True)
      )
    )
    (5): InvertedResidual(
      (branch1): Sequential()
      (branch2): Sequential(
        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (7): ReLU(inplace=True)
      )
    )
    (6): InvertedResidual(
      (branch1): Sequential()
      (branch2): Sequential(
        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (7): ReLU(inplace=True)
      )
    )
    (7): InvertedResidual(
      (branch1): Sequential()
      (branch2): Sequential(
        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (7): ReLU(inplace=True)
      )
    )
  )
  (stage4): Sequential(
    (0): InvertedResidual(
      (branch1): Sequential(
        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (4): ReLU(inplace=True)
      )
      (branch2): Sequential(
        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (7): ReLU(inplace=True)
      )
    )
    (1): InvertedResidual(
      (branch1): Sequential()
      (branch2): Sequential(
        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (7): ReLU(inplace=True)
      )
    )
    (2): InvertedResidual(
      (branch1): Sequential()
      (branch2): Sequential(
        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (7): ReLU(inplace=True)
      )
    )
    (3): InvertedResidual(
      (branch1): Sequential()
      (branch2): Sequential(
        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (7): ReLU(inplace=True)
      )
    )
  )
  (conv5): Sequential(
    (0): Conv2d(192, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=1024, out_features=3, bias=True)
  )
)
Batch 1, Train Loss:0.037615, Train ACC:0.3333
Batch 2, Train Loss:0.037187, Train ACC:0.3500
Batch 3, Train Loss:0.037277, Train ACC:0.3333
Batch 4, Train Loss:0.036996, Train ACC:0.3250
Batch 5, Train Loss:0.037208, Train ACC:0.3067
Batch 6, Train Loss:0.036709, Train ACC:0.3500
Batch 7, Train Loss:0.036241, Train ACC:0.3810
Batch 8, Train Loss:0.035811, Train ACC:0.4167
Batch 9, Train Loss:0.035472, Train ACC:0.4815
Batch 10, Train Loss:0.035043, Train ACC:0.5333
Batch 11, Train Loss:0.034689, Train ACC:0.5485
Batch 12, Train Loss:0.034253, Train ACC:0.5583
Batch 13, Train Loss:0.033821, Train ACC:0.5923
Batch 14, Train Loss:0.033660, Train ACC:0.5667
Batch 15, Train Loss:0.033311, Train ACC:0.5800
Batch 16, Train Loss:0.032977, Train ACC:0.6062
Batch 17, Train Loss:0.032636, Train ACC:0.6216
Batch 18, Train Loss:0.032537, Train ACC:0.6333
Batch 19, Train Loss:0.032287, Train ACC:0.6316
Batch 20, Train Loss:0.032061, Train ACC:0.6267
Batch 21, Train Loss:0.031631, Train ACC:0.6444
Batch 22, Train Loss:0.031408, Train ACC:0.6606
Batch 23, Train Loss:0.031046, Train ACC:0.6754
Batch 24, Train Loss:0.030678, Train ACC:0.6889
Batch 25, Train Loss:0.030331, Train ACC:0.7013
Batch 26, Train Loss:0.029963, Train ACC:0.7128
Batch 27, Train Loss:0.029622, Train ACC:0.7235
Batch 28, Train Loss:0.029221, Train ACC:0.7333
Batch 29, Train Loss:0.028869, Train ACC:0.7425
Batch 30, Train Loss:0.028587, Train ACC:0.7511
Batch 31, Train Loss:0.028202, Train ACC:0.7591
Batch 32, Train Loss:0.027992, Train ACC:0.7667
Batch 33, Train Loss:0.027705, Train ACC:0.7737
Batch 34, Train Loss:0.027515, Train ACC:0.7804
Batch 35, Train Loss:0.027475, Train ACC:0.7867
Batch 36, Train Loss:0.027181, Train ACC:0.7926
Batch 37, Train Loss:0.026851, Train ACC:0.7982
Batch 38, Train Loss:0.026586, Train ACC:0.8035
Batch 39, Train Loss:0.026333, Train ACC:0.8085
Batch 40, Train Loss:0.026089, Train ACC:0.8133
Batch 41, Train Loss:0.025830, Train ACC:0.8179
Batch 42, Train Loss:0.025544, Train ACC:0.8222
Batch 43, Train Loss:0.025248, Train ACC:0.8264
Batch 44, Train Loss:0.024977, Train ACC:0.8303
Batch 45, Train Loss:0.024728, Train ACC:0.8341
Batch 46, Train Loss:0.024477, Train ACC:0.8377
Batch 47, Train Loss:0.024397, Train ACC:0.8411
Batch 48, Train Loss:0.024130, Train ACC:0.8444
Batch 49, Train Loss:0.023848, Train ACC:0.8476
Batch 50, Train Loss:0.023695, Train ACC:0.8507
Batch 51, Train Loss:0.023513, Train ACC:0.8536
Batch 52, Train Loss:0.023309, Train ACC:0.8564
Batch 53, Train Loss:0.023086, Train ACC:0.8591
Batch 54, Train Loss:0.022853, Train ACC:0.8617
Batch 55, Train Loss:0.022704, Train ACC:0.8642
Batch 56, Train Loss:0.022507, Train ACC:0.8667
Batch 57, Train Loss:0.022286, Train ACC:0.8690
Batch 58, Train Loss:0.022082, Train ACC:0.8713
Batch 59, Train Loss:0.022065, Train ACC:0.8734
Batch 60, Train Loss:0.021886, Train ACC:0.8756
Batch 61, Train Loss:0.021684, Train ACC:0.8776
Batch 62, Train Loss:0.021470, Train ACC:0.8796
Batch 63, Train Loss:0.021320, Train ACC:0.8815
Batch 64, Train Loss:0.021118, Train ACC:0.8833
Batch 65, Train Loss:0.020932, Train ACC:0.8851
Batch 66, Train Loss:0.020771, Train ACC:0.8869
Batch 67, Train Loss:0.020601, Train ACC:0.8886
Batch 68, Train Loss:0.020410, Train ACC:0.8902
Batch 69, Train Loss:0.020236, Train ACC:0.8918
Batch 70, Train Loss:0.020049, Train ACC:0.8933
Batch 71, Train Loss:0.019929, Train ACC:0.8948
Batch 72, Train Loss:0.019728, Train ACC:0.8963
Batch 73, Train Loss:0.019578, Train ACC:0.8977
Batch 74, Train Loss:0.019428, Train ACC:0.8991
Batch 75, Train Loss:0.019238, Train ACC:0.9004
Batch 76, Train Loss:0.019056, Train ACC:0.9018
Batch 77, Train Loss:0.018897, Train ACC:0.9030
Batch 78, Train Loss:0.018723, Train ACC:0.9043
Batch 79, Train Loss:0.018545, Train ACC:0.9055
Batch 80, Train Loss:0.018366, Train ACC:0.9067
Batch 81, Train Loss:0.018199, Train ACC:0.9078
Batch 82, Train Loss:0.018040, Train ACC:0.9089
Batch 83, Train Loss:0.017867, Train ACC:0.9100
Batch 84, Train Loss:0.017709, Train ACC:0.9111
Batch 85, Train Loss:0.017549, Train ACC:0.9122
Batch 86, Train Loss:0.017386, Train ACC:0.9132
Batch 87, Train Loss:0.017246, Train ACC:0.9142
Batch 88, Train Loss:0.017099, Train ACC:0.9152
Batch 89, Train Loss:0.016940, Train ACC:0.9161
Batch 90, Train Loss:0.016791, Train ACC:0.9170
Batch 91, Train Loss:0.016657, Train ACC:0.9179
Batch 92, Train Loss:0.016518, Train ACC:0.9188
Batch 93, Train Loss:0.016374, Train ACC:0.9197
Batch 94, Train Loss:0.016248, Train ACC:0.9206
Batch 95, Train Loss:0.016185, Train ACC:0.9214
Batch 96, Train Loss:0.016067, Train ACC:0.9222
Batch 97, Train Loss:0.015939, Train ACC:0.9230
Batch 98, Train Loss:0.015821, Train ACC:0.9238
Batch 99, Train Loss:0.015748, Train ACC:0.9246
Batch 100, Train Loss:0.015684, Train ACC:0.9253
Batch 101, Train Loss:0.015608, Train ACC:0.9261
Batch 102, Train Loss:0.015526, Train ACC:0.9268
Batch 103, Train Loss:0.015403, Train ACC:0.9275
Batch 104, Train Loss:0.015298, Train ACC:0.9282
Batch 105, Train Loss:0.015180, Train ACC:0.9289
Batch 106, Train Loss:0.015066, Train ACC:0.9296
Batch 107, Train Loss:0.014990, Train ACC:0.9302
Batch 108, Train Loss:0.014885, Train ACC:0.9309
Batch 109, Train Loss:0.014774, Train ACC:0.9315
Batch 110, Train Loss:0.014687, Train ACC:0.9321
Batch 111, Train Loss:0.014591, Train ACC:0.9327
Batch 112, Train Loss:0.014497, Train ACC:0.9333
Batch 113, Train Loss:0.014387, Train ACC:0.9339
Batch 114, Train Loss:0.014283, Train ACC:0.9345
Batch 115, Train Loss:0.014253, Train ACC:0.9351
Batch 116, Train Loss:0.014189, Train ACC:0.9356
Batch 117, Train Loss:0.014112, Train ACC:0.9362
Batch 118, Train Loss:0.014056, Train ACC:0.9367
Batch 119, Train Loss:0.013957, Train ACC:0.9373
Batch 120, Train Loss:0.013858, Train ACC:0.9378
Batch 121, Train Loss:0.013760, Train ACC:0.9383
Batch 122, Train Loss:0.013750, Train ACC:0.9388
Batch 123, Train Loss:0.013664, Train ACC:0.9393
Batch 124, Train Loss:0.013577, Train ACC:0.9398
Batch 125, Train Loss:0.013493, Train ACC:0.9403
Batch 126, Train Loss:0.013420, Train ACC:0.9407
Batch 127, Train Loss:0.013333, Train ACC:0.9412
Batch 128, Train Loss:0.013246, Train ACC:0.9417
Batch 129, Train Loss:0.013174, Train ACC:0.9421
Batch 130, Train Loss:0.013103, Train ACC:0.9426
Batch 131, Train Loss:0.013021, Train ACC:0.9430
Batch 132, Train Loss:0.012939, Train ACC:0.9434
Batch 133, Train Loss:0.012862, Train ACC:0.9439
Batch 134, Train Loss:0.012800, Train ACC:0.9443
Batch 135, Train Loss:0.012724, Train ACC:0.9447
Batch 136, Train Loss:0.012701, Train ACC:0.9451
Batch 137, Train Loss:0.012621, Train ACC:0.9455
Batch 138, Train Loss:0.012541, Train ACC:0.9459
Batch 139, Train Loss:0.012462, Train ACC:0.9463
Batch 140, Train Loss:0.012385, Train ACC:0.9467
Batch 141, Train Loss:0.012310, Train ACC:0.9470
Batch 142, Train Loss:0.012240, Train ACC:0.9474
Batch 143, Train Loss:0.012168, Train ACC:0.9478
Batch 144, Train Loss:0.012101, Train ACC:0.9481
Batch 145, Train Loss:0.012029, Train ACC:0.9485
Batch 146, Train Loss:0.011959, Train ACC:0.9489
Batch 147, Train Loss:0.011895, Train ACC:0.9492
Batch 148, Train Loss:0.011824, Train ACC:0.9495
Batch 149, Train Loss:0.011770, Train ACC:0.9499
Batch 150, Train Loss:0.011701, Train ACC:0.9502
Batch 151, Train Loss:0.011632, Train ACC:0.9506
Batch 152, Train Loss:0.011563, Train ACC:0.9509
Batch 153, Train Loss:0.011512, Train ACC:0.9512
Batch 154, Train Loss:0.011444, Train ACC:0.9515
Batch 155, Train Loss:0.011393, Train ACC:0.9518
Batch 156, Train Loss:0.011336, Train ACC:0.9521
Batch 157, Train Loss:0.011275, Train ACC:0.9524
Batch 158, Train Loss:0.011219, Train ACC:0.9527
Batch 159, Train Loss:0.011172, Train ACC:0.9530
Batch 160, Train Loss:0.011109, Train ACC:0.9533
Batch 161, Train Loss:0.011046, Train ACC:0.9536
Batch 162, Train Loss:0.010992, Train ACC:0.9539
Batch 163, Train Loss:0.010931, Train ACC:0.9542
Batch 164, Train Loss:0.010876, Train ACC:0.9545
Batch 165, Train Loss:0.010825, Train ACC:0.9547
Batch 166, Train Loss:0.010781, Train ACC:0.9550
Batch 167, Train Loss:0.010724, Train ACC:0.9553
Batch 168, Train Loss:0.010702, Train ACC:0.9556
Batch 169, Train Loss:0.010650, Train ACC:0.9558
Batch 170, Train Loss:0.010593, Train ACC:0.9561
Batch 171, Train Loss:0.010542, Train ACC:0.9563
Batch 172, Train Loss:0.010489, Train ACC:0.9566
Batch 173, Train Loss:0.010435, Train ACC:0.9568
Batch 174, Train Loss:0.010383, Train ACC:0.9571
Batch 175, Train Loss:0.010330, Train ACC:0.9573
Batch 176, Train Loss:0.010277, Train ACC:0.9576
Batch 177, Train Loss:0.010241, Train ACC:0.9578
Batch 178, Train Loss:0.010213, Train ACC:0.9581
Batch 179, Train Loss:0.010162, Train ACC:0.9583
Batch 180, Train Loss:0.010111, Train ACC:0.9585
Batch 181, Train Loss:0.010067, Train ACC:0.9587
Batch 182, Train Loss:0.010018, Train ACC:0.9590
Batch 183, Train Loss:0.009969, Train ACC:0.9592
Batch 184, Train Loss:0.009923, Train ACC:0.9594
Batch 185, Train Loss:0.009876, Train ACC:0.9596
Batch 186, Train Loss:0.009828, Train ACC:0.9599
Batch 187, Train Loss:0.009780, Train ACC:0.9601
Batch 188, Train Loss:0.009733, Train ACC:0.9603
Batch 189, Train Loss:0.009701, Train ACC:0.9605
Batch 190, Train Loss:0.009672, Train ACC:0.9607
Batch 191, Train Loss:0.009625, Train ACC:0.9609
Batch 192, Train Loss:0.009580, Train ACC:0.9611
Batch 193, Train Loss:0.009536, Train ACC:0.9613
Batch 194, Train Loss:0.009490, Train ACC:0.9615
Batch 195, Train Loss:0.009445, Train ACC:0.9617
Batch 196, Train Loss:0.009406, Train ACC:0.9619
Batch 197, Train Loss:0.009362, Train ACC:0.9621
Batch 198, Train Loss:0.009330, Train ACC:0.9623
Batch 199, Train Loss:0.009310, Train ACC:0.9625
Batch 200, Train Loss:0.009275, Train ACC:0.9627
Batch 201, Train Loss:0.009244, Train ACC:0.9629
Batch 202, Train Loss:0.009202, Train ACC:0.9630
Batch 203, Train Loss:0.009199, Train ACC:0.9632
Batch 204, Train Loss:0.009157, Train ACC:0.9634
Batch 205, Train Loss:0.009123, Train ACC:0.9636
Batch 206, Train Loss:0.009088, Train ACC:0.9638
Batch 207, Train Loss:0.009052, Train ACC:0.9639
Batch 208, Train Loss:0.009012, Train ACC:0.9641
Batch 209, Train Loss:0.008973, Train ACC:0.9643
Batch 210, Train Loss:0.008936, Train ACC:0.9644
Batch 211, Train Loss:0.008904, Train ACC:0.9646
Batch 212, Train Loss:0.008866, Train ACC:0.9648
Batch 213, Train Loss:0.008829, Train ACC:0.9649
Batch 214, Train Loss:0.008794, Train ACC:0.9651
Batch 215, Train Loss:0.008756, Train ACC:0.9653
Batch 216, Train Loss:0.008735, Train ACC:0.9654
Batch 217, Train Loss:0.008698, Train ACC:0.9656
Batch 218, Train Loss:0.008664, Train ACC:0.9657
Batch 219, Train Loss:0.008629, Train ACC:0.9659
Batch 220, Train Loss:0.008604, Train ACC:0.9661
Batch 221, Train Loss:0.008569, Train ACC:0.9662
Batch 222, Train Loss:0.008541, Train ACC:0.9664
Batch 223, Train Loss:0.008509, Train ACC:0.9665
Batch 224, Train Loss:0.008474, Train ACC:0.9667
Batch 225, Train Loss:0.008439, Train ACC:0.9668
Batch 226, Train Loss:0.008405, Train ACC:0.9670
Batch 227, Train Loss:0.008373, Train ACC:0.9671
Batch 228, Train Loss:0.008343, Train ACC:0.9673
Batch 229, Train Loss:0.008312, Train ACC:0.9674
Batch 230, Train Loss:0.008281, Train ACC:0.9675
Batch 231, Train Loss:0.008250, Train ACC:0.9677
Batch 232, Train Loss:0.008217, Train ACC:0.9678
Batch 233, Train Loss:0.008184, Train ACC:0.9680
Batch 234, Train Loss:0.008151, Train ACC:0.9681
Batch 235, Train Loss:0.008119, Train ACC:0.9682
Batch 236, Train Loss:0.008090, Train ACC:0.9684
Batch 237, Train Loss:0.008058, Train ACC:0.9685
Batch 238, Train Loss:0.008035, Train ACC:0.9686
Batch 239, Train Loss:0.008004, Train ACC:0.9688
Batch 240, Train Loss:0.007974, Train ACC:0.9689
Batch 241, Train Loss:0.007943, Train ACC:0.9690
Batch 242, Train Loss:0.007913, Train ACC:0.9691
Batch 243, Train Loss:0.007883, Train ACC:0.9693
Batch 244, Train Loss:0.007855, Train ACC:0.9694
Batch 245, Train Loss:0.007825, Train ACC:0.9695
Batch 246, Train Loss:0.007795, Train ACC:0.9696
Batch 247, Train Loss:0.007766, Train ACC:0.9698
Batch 248, Train Loss:0.007737, Train ACC:0.9699
Batch 249, Train Loss:0.007708, Train ACC:0.9700
Batch 250, Train Loss:0.007679, Train ACC:0.9701
Batch 251, Train Loss:0.007656, Train ACC:0.9703
Batch 252, Train Loss:0.007631, Train ACC:0.9704
Batch 253, Train Loss:0.007603, Train ACC:0.9705
Batch 254, Train Loss:0.007577, Train ACC:0.9706
Batch 255, Train Loss:0.007549, Train ACC:0.9707
Batch 256, Train Loss:0.007521, Train ACC:0.9708
Batch 257, Train Loss:0.007494, Train ACC:0.9709
Batch 258, Train Loss:0.007466, Train ACC:0.9711
Batch 259, Train Loss:0.007439, Train ACC:0.9712
Batch 260, Train Loss:0.007412, Train ACC:0.9713
Batch 261, Train Loss:0.007386, Train ACC:0.9714
Batch 262, Train Loss:0.007359, Train ACC:0.9715
Batch 263, Train Loss:0.007339, Train ACC:0.9716
Batch 264, Train Loss:0.007313, Train ACC:0.9717
Batch 265, Train Loss:0.007288, Train ACC:0.9718
Batch 266, Train Loss:0.007268, Train ACC:0.9719
Batch 267, Train Loss:0.007243, Train ACC:0.9720
Batch 268, Train Loss:0.007218, Train ACC:0.9721
Batch 269, Train Loss:0.007192, Train ACC:0.9722
Batch 270, Train Loss:0.007167, Train ACC:0.9723
Batch 271, Train Loss:0.007148, Train ACC:0.9724
Batch 272, Train Loss:0.007124, Train ACC:0.9725
Batch 273, Train Loss:0.007100, Train ACC:0.9726
Batch 274, Train Loss:0.007076, Train ACC:0.9727
Batch 275, Train Loss:0.007052, Train ACC:0.9728
Batch 276, Train Loss:0.007028, Train ACC:0.9729
Batch 277, Train Loss:0.007005, Train ACC:0.9730
Batch 278, Train Loss:0.006982, Train ACC:0.9731
Batch 279, Train Loss:0.006959, Train ACC:0.9732
Batch 280, Train Loss:0.006946, Train ACC:0.9733
Batch 281, Train Loss:0.006923, Train ACC:0.9734
Batch 282, Train Loss:0.006900, Train ACC:0.9735
Batch 283, Train Loss:0.006877, Train ACC:0.9736
Batch 284, Train Loss:0.006855, Train ACC:0.9737
Batch 285, Train Loss:0.006832, Train ACC:0.9738
Batch 286, Train Loss:0.006816, Train ACC:0.9739
Batch 287, Train Loss:0.006793, Train ACC:0.9740
Batch 288, Train Loss:0.006772, Train ACC:0.9741
Batch 289, Train Loss:0.006752, Train ACC:0.9742
Batch 290, Train Loss:0.006730, Train ACC:0.9743
Batch 291, Train Loss:0.006707, Train ACC:0.9743
Batch 292, Train Loss:0.006688, Train ACC:0.9744
Batch 293, Train Loss:0.006666, Train ACC:0.9745
Batch 294, Train Loss:0.006650, Train ACC:0.9746
Batch 295, Train Loss:0.006649, Train ACC:0.9747
Batch 296, Train Loss:0.006627, Train ACC:0.9748
Batch 297, Train Loss:0.006606, Train ACC:0.9749
Batch 298, Train Loss:0.006586, Train ACC:0.9749
Batch 299, Train Loss:0.006568, Train ACC:0.9750
Batch 300, Train Loss:0.006550, Train ACC:0.9751
预测标签：tensor([0, 1, 0, 2, 0, 2, 0, 1, 0, 0, 1, 0, 1, 1, 0, 2, 2, 0, 0, 0, 0, 0, 2, 2,
        2, 0, 1, 2, 2, 2], device='cuda:0'), 真实标签：tensor([0, 1, 0, 2, 0, 2, 0, 1, 0, 0, 1, 0, 1, 1, 0, 2, 2, 0, 0, 0, 0, 0, 2, 2,
        2, 0, 1, 2, 2, 2], device='cuda:0')
SqueezeNet(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(1, 96, kernel_size=(7, 7), stride=(2, 2))
    )
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
    (3): Fire(
      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))
      (squeeze_activation): ReLU(inplace=True)
      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
      (expand1x1_activation): ReLU(inplace=True)
      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (expand3x3_activation): ReLU(inplace=True)
    )
    (4): Fire(
      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))
      (squeeze_activation): ReLU(inplace=True)
      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
      (expand1x1_activation): ReLU(inplace=True)
      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (expand3x3_activation): ReLU(inplace=True)
    )
    (5): Fire(
      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
      (squeeze_activation): ReLU(inplace=True)
      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
      (expand1x1_activation): ReLU(inplace=True)
      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (expand3x3_activation): ReLU(inplace=True)
    )
    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
    (7): Fire(
      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
      (squeeze_activation): ReLU(inplace=True)
      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
      (expand1x1_activation): ReLU(inplace=True)
      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (expand3x3_activation): ReLU(inplace=True)
    )
    (8): Fire(
      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))
      (squeeze_activation): ReLU(inplace=True)
      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
      (expand1x1_activation): ReLU(inplace=True)
      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (expand3x3_activation): ReLU(inplace=True)
    )
    (9): Fire(
      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))
      (squeeze_activation): ReLU(inplace=True)
      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
      (expand1x1_activation): ReLU(inplace=True)
      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (expand3x3_activation): ReLU(inplace=True)
    )
    (10): Fire(
      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))
      (squeeze_activation): ReLU(inplace=True)
      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
      (expand1x1_activation): ReLU(inplace=True)
      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (expand3x3_activation): ReLU(inplace=True)
    )
    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
    (12): Fire(
      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))
      (squeeze_activation): ReLU(inplace=True)
      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
      (expand1x1_activation): ReLU(inplace=True)
      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (expand3x3_activation): ReLU(inplace=True)
    )
  )
  (classifier): Sequential(
    (0): Dropout(p=0.5, inplace=False)
    (1): Conv2d(512, 3, kernel_size=(1, 1), stride=(1, 1))
    (2): ReLU(inplace=True)
    (3): AdaptiveAvgPool2d(output_size=(1, 1))
  )
)
Batch 1, Train Loss:0.038972, Train ACC:0.3000
Batch 2, Train Loss:0.038084, Train ACC:0.3333
Batch 3, Train Loss:0.037632, Train ACC:0.3444
Batch 4, Train Loss:0.037374, Train ACC:0.3250
Batch 5, Train Loss:0.037196, Train ACC:0.3267
Batch 6, Train Loss:0.036955, Train ACC:0.3444
Batch 7, Train Loss:0.036763, Train ACC:0.3524
Batch 8, Train Loss:0.036672, Train ACC:0.3500
Batch 9, Train Loss:0.036702, Train ACC:0.3333
Batch 10, Train Loss:0.036597, Train ACC:0.3400
Batch 11, Train Loss:0.036531, Train ACC:0.3394
Batch 12, Train Loss:0.036597, Train ACC:0.3389
Batch 13, Train Loss:0.036625, Train ACC:0.3359
Batch 14, Train Loss:0.036661, Train ACC:0.3286
Batch 15, Train Loss:0.036570, Train ACC:0.3333
Batch 16, Train Loss:0.036548, Train ACC:0.3333
Batch 17, Train Loss:0.036448, Train ACC:0.3373
Batch 18, Train Loss:0.036350, Train ACC:0.3444
Batch 19, Train Loss:0.036275, Train ACC:0.3509
Batch 20, Train Loss:0.036211, Train ACC:0.3517
Batch 21, Train Loss:0.036163, Train ACC:0.3524
Batch 22, Train Loss:0.036151, Train ACC:0.3485
Batch 23, Train Loss:0.036128, Train ACC:0.3478
Batch 24, Train Loss:0.036126, Train ACC:0.3458
Batch 25, Train Loss:0.036036, Train ACC:0.3480
Batch 26, Train Loss:0.035939, Train ACC:0.3500
Batch 27, Train Loss:0.035886, Train ACC:0.3494
Batch 28, Train Loss:0.035795, Train ACC:0.3512
Batch 29, Train Loss:0.035753, Train ACC:0.3506
Batch 30, Train Loss:0.035730, Train ACC:0.3489
Batch 31, Train Loss:0.035692, Train ACC:0.3484
Batch 32, Train Loss:0.035613, Train ACC:0.3510
Batch 33, Train Loss:0.035521, Train ACC:0.3515
Batch 34, Train Loss:0.035538, Train ACC:0.3480
Batch 35, Train Loss:0.035423, Train ACC:0.3524
Batch 36, Train Loss:0.035339, Train ACC:0.3537
Batch 37, Train Loss:0.035335, Train ACC:0.3505
Batch 38, Train Loss:0.035326, Train ACC:0.3482
Batch 39, Train Loss:0.035312, Train ACC:0.3444
Batch 40, Train Loss:0.035249, Train ACC:0.3433
Batch 41, Train Loss:0.035166, Train ACC:0.3431
Batch 42, Train Loss:0.035052, Train ACC:0.3452
Batch 43, Train Loss:0.034988, Train ACC:0.3465
Batch 44, Train Loss:0.034924, Train ACC:0.3470
Batch 45, Train Loss:0.034804, Train ACC:0.3489
Batch 46, Train Loss:0.034733, Train ACC:0.3486
Batch 47, Train Loss:0.034713, Train ACC:0.3461
Batch 48, Train Loss:0.034580, Train ACC:0.3486
Batch 49, Train Loss:0.034530, Train ACC:0.3463
Batch 50, Train Loss:0.034460, Train ACC:0.3473
Batch 51, Train Loss:0.034462, Train ACC:0.3444
Batch 52, Train Loss:0.034347, Train ACC:0.3462
Batch 53, Train Loss:0.034329, Train ACC:0.3428
Batch 54, Train Loss:0.034262, Train ACC:0.3432
Batch 55, Train Loss:0.034119, Train ACC:0.3473
Batch 56, Train Loss:0.034058, Train ACC:0.3470
Batch 57, Train Loss:0.033985, Train ACC:0.3468
Batch 58, Train Loss:0.033896, Train ACC:0.3477
Batch 59, Train Loss:0.033869, Train ACC:0.3463
Batch 60, Train Loss:0.033764, Train ACC:0.3472
Batch 61, Train Loss:0.033738, Train ACC:0.3454
Batch 62, Train Loss:0.033615, Train ACC:0.3468
Batch 63, Train Loss:0.033494, Train ACC:0.3481
Batch 64, Train Loss:0.033438, Train ACC:0.3474
Batch 65, Train Loss:0.033328, Train ACC:0.3487
Batch 66, Train Loss:0.033293, Train ACC:0.3475
Batch 67, Train Loss:0.033215, Train ACC:0.3483
Batch 68, Train Loss:0.033119, Train ACC:0.3485
Batch 69, Train Loss:0.033079, Train ACC:0.3473
Batch 70, Train Loss:0.033031, Train ACC:0.3467
Batch 71, Train Loss:0.033026, Train ACC:0.3446
Batch 72, Train Loss:0.033010, Train ACC:0.3431
Batch 73, Train Loss:0.032930, Train ACC:0.3438
Batch 74, Train Loss:0.032872, Train ACC:0.3428
Batch 75, Train Loss:0.032832, Train ACC:0.3413
Batch 76, Train Loss:0.032774, Train ACC:0.3408
Batch 77, Train Loss:0.032702, Train ACC:0.3411
Batch 78, Train Loss:0.032615, Train ACC:0.3427
Batch 79, Train Loss:0.032522, Train ACC:0.3435
Batch 80, Train Loss:0.032465, Train ACC:0.3429
Batch 81, Train Loss:0.032440, Train ACC:0.3416
Batch 82, Train Loss:0.032346, Train ACC:0.3423
Batch 83, Train Loss:0.032268, Train ACC:0.3422
Batch 84, Train Loss:0.032182, Train ACC:0.3429
Batch 85, Train Loss:0.032078, Train ACC:0.3439
Batch 86, Train Loss:0.032033, Train ACC:0.3430
Batch 87, Train Loss:0.031957, Train ACC:0.3433
Batch 88, Train Loss:0.031879, Train ACC:0.3439
Batch 89, Train Loss:0.031848, Train ACC:0.3423
Batch 90, Train Loss:0.031785, Train ACC:0.3422
Batch 91, Train Loss:0.031693, Train ACC:0.3432
Batch 92, Train Loss:0.031610, Train ACC:0.3435
Batch 93, Train Loss:0.031571, Train ACC:0.3423
Batch 94, Train Loss:0.031492, Train ACC:0.3457
Batch 95, Train Loss:0.031430, Train ACC:0.3491
Batch 96, Train Loss:0.031304, Train ACC:0.3535
Batch 97, Train Loss:0.031234, Train ACC:0.3567
Batch 98, Train Loss:0.031146, Train ACC:0.3599
Batch 99, Train Loss:0.031091, Train ACC:0.3630
Batch 100, Train Loss:0.031021, Train ACC:0.3667
Batch 101, Train Loss:0.030947, Train ACC:0.3713
Batch 102, Train Loss:0.030888, Train ACC:0.3745
Batch 103, Train Loss:0.030773, Train ACC:0.3786
Batch 104, Train Loss:0.030742, Train ACC:0.3798
Batch 105, Train Loss:0.030656, Train ACC:0.3829
Batch 106, Train Loss:0.030572, Train ACC:0.3862
Batch 107, Train Loss:0.030495, Train ACC:0.3894
Batch 108, Train Loss:0.030409, Train ACC:0.3923
Batch 109, Train Loss:0.030359, Train ACC:0.3945
Batch 110, Train Loss:0.030333, Train ACC:0.3964
Batch 111, Train Loss:0.030277, Train ACC:0.3985
Batch 112, Train Loss:0.030202, Train ACC:0.4015
Batch 113, Train Loss:0.030093, Train ACC:0.4050
Batch 114, Train Loss:0.030037, Train ACC:0.4070
Batch 115, Train Loss:0.029962, Train ACC:0.4096
Batch 116, Train Loss:0.029902, Train ACC:0.4115
Batch 117, Train Loss:0.029832, Train ACC:0.4137
Batch 118, Train Loss:0.029783, Train ACC:0.4155
Batch 119, Train Loss:0.029712, Train ACC:0.4174
Batch 120, Train Loss:0.029672, Train ACC:0.4192
Batch 121, Train Loss:0.029589, Train ACC:0.4215
Batch 122, Train Loss:0.029499, Train ACC:0.4246
Batch 123, Train Loss:0.029431, Train ACC:0.4266
Batch 124, Train Loss:0.029376, Train ACC:0.4280
Batch 125, Train Loss:0.029311, Train ACC:0.4301
Batch 126, Train Loss:0.029264, Train ACC:0.4312
Batch 127, Train Loss:0.029202, Train ACC:0.4331
Batch 128, Train Loss:0.029127, Train ACC:0.4352
Batch 129, Train Loss:0.029045, Train ACC:0.4372
Batch 130, Train Loss:0.028999, Train ACC:0.4382
Batch 131, Train Loss:0.028926, Train ACC:0.4402
Batch 132, Train Loss:0.028834, Train ACC:0.4424
Batch 133, Train Loss:0.028765, Train ACC:0.4444
Batch 134, Train Loss:0.028702, Train ACC:0.4465
Batch 135, Train Loss:0.028642, Train ACC:0.4479
Batch 136, Train Loss:0.028560, Train ACC:0.4500
Batch 137, Train Loss:0.028492, Train ACC:0.4516
Batch 138, Train Loss:0.028387, Train ACC:0.4546
Batch 139, Train Loss:0.028311, Train ACC:0.4571
Batch 140, Train Loss:0.028237, Train ACC:0.4588
Batch 141, Train Loss:0.028159, Train ACC:0.4608
Batch 142, Train Loss:0.028093, Train ACC:0.4624
Batch 143, Train Loss:0.028028, Train ACC:0.4639
Batch 144, Train Loss:0.027979, Train ACC:0.4650
Batch 145, Train Loss:0.027902, Train ACC:0.4669
Batch 146, Train Loss:0.027818, Train ACC:0.4689
Batch 147, Train Loss:0.027739, Train ACC:0.4707
Batch 148, Train Loss:0.027687, Train ACC:0.4718
Batch 149, Train Loss:0.027634, Train ACC:0.4729
Batch 150, Train Loss:0.027599, Train ACC:0.4736
Batch 151, Train Loss:0.027542, Train ACC:0.4748
Batch 152, Train Loss:0.027477, Train ACC:0.4763
Batch 153, Train Loss:0.027402, Train ACC:0.4782
Batch 154, Train Loss:0.027316, Train ACC:0.4803
Batch 155, Train Loss:0.027262, Train ACC:0.4815
Batch 156, Train Loss:0.027210, Train ACC:0.4825
Batch 157, Train Loss:0.027159, Train ACC:0.4834
Batch 158, Train Loss:0.027124, Train ACC:0.4840
Batch 159, Train Loss:0.027052, Train ACC:0.4855
Batch 160, Train Loss:0.026995, Train ACC:0.4867
Batch 161, Train Loss:0.026917, Train ACC:0.4884
Batch 162, Train Loss:0.026838, Train ACC:0.4901
Batch 163, Train Loss:0.026780, Train ACC:0.4912
Batch 164, Train Loss:0.026764, Train ACC:0.4911
Batch 165, Train Loss:0.026687, Train ACC:0.4927
Batch 166, Train Loss:0.026620, Train ACC:0.4940
Batch 167, Train Loss:0.026576, Train ACC:0.4946
Batch 168, Train Loss:0.026538, Train ACC:0.4952
Batch 169, Train Loss:0.026470, Train ACC:0.4968
Batch 170, Train Loss:0.026434, Train ACC:0.4973
Batch 171, Train Loss:0.026421, Train ACC:0.4971
Batch 172, Train Loss:0.026359, Train ACC:0.4981
Batch 173, Train Loss:0.026289, Train ACC:0.4996
Batch 174, Train Loss:0.026241, Train ACC:0.5004
Batch 175, Train Loss:0.026193, Train ACC:0.5011
Batch 176, Train Loss:0.026112, Train ACC:0.5028
Batch 177, Train Loss:0.026030, Train ACC:0.5045
Batch 178, Train Loss:0.025988, Train ACC:0.5051
Batch 179, Train Loss:0.025925, Train ACC:0.5061
Batch 180, Train Loss:0.025902, Train ACC:0.5061
Batch 181, Train Loss:0.025821, Train ACC:0.5077
Batch 182, Train Loss:0.025778, Train ACC:0.5084
Batch 183, Train Loss:0.025741, Train ACC:0.5087
Batch 184, Train Loss:0.025675, Train ACC:0.5100
Batch 185, Train Loss:0.025646, Train ACC:0.5101
Batch 186, Train Loss:0.025603, Train ACC:0.5108
Batch 187, Train Loss:0.025535, Train ACC:0.5121
Batch 188, Train Loss:0.025512, Train ACC:0.5121
Batch 189, Train Loss:0.025461, Train ACC:0.5129
Batch 190, Train Loss:0.025410, Train ACC:0.5137
Batch 191, Train Loss:0.025363, Train ACC:0.5143
Batch 192, Train Loss:0.025325, Train ACC:0.5148
Batch 193, Train Loss:0.025288, Train ACC:0.5152
Batch 194, Train Loss:0.025234, Train ACC:0.5162
Batch 195, Train Loss:0.025173, Train ACC:0.5173
Batch 196, Train Loss:0.025118, Train ACC:0.5182
Batch 197, Train Loss:0.025082, Train ACC:0.5186
Batch 198, Train Loss:0.025029, Train ACC:0.5195
Batch 199, Train Loss:0.025004, Train ACC:0.5196
Batch 200, Train Loss:0.024956, Train ACC:0.5203
Batch 201, Train Loss:0.024920, Train ACC:0.5207
Batch 202, Train Loss:0.024873, Train ACC:0.5215
Batch 203, Train Loss:0.024827, Train ACC:0.5222
Batch 204, Train Loss:0.024758, Train ACC:0.5235
Batch 205, Train Loss:0.024723, Train ACC:0.5239
Batch 206, Train Loss:0.024688, Train ACC:0.5243
Batch 207, Train Loss:0.024636, Train ACC:0.5251
Batch 208, Train Loss:0.024584, Train ACC:0.5260
Batch 209, Train Loss:0.024561, Train ACC:0.5260
Batch 210, Train Loss:0.024522, Train ACC:0.5265
Batch 211, Train Loss:0.024469, Train ACC:0.5273
Batch 212, Train Loss:0.024424, Train ACC:0.5280
Batch 213, Train Loss:0.024384, Train ACC:0.5285
Batch 214, Train Loss:0.024346, Train ACC:0.5290
Batch 215, Train Loss:0.024304, Train ACC:0.5296
Batch 216, Train Loss:0.024270, Train ACC:0.5299
Batch 217, Train Loss:0.024242, Train ACC:0.5301
Batch 218, Train Loss:0.024177, Train ACC:0.5313
Batch 219, Train Loss:0.024119, Train ACC:0.5324
Batch 220, Train Loss:0.024067, Train ACC:0.5333
Batch 221, Train Loss:0.024046, Train ACC:0.5333
Batch 222, Train Loss:0.024013, Train ACC:0.5336
Batch 223, Train Loss:0.023977, Train ACC:0.5341
Batch 224, Train Loss:0.023908, Train ACC:0.5354
Batch 225, Train Loss:0.023867, Train ACC:0.5360
Batch 226, Train Loss:0.023827, Train ACC:0.5366
Batch 227, Train Loss:0.023798, Train ACC:0.5369
Batch 228, Train Loss:0.023767, Train ACC:0.5371
Batch 229, Train Loss:0.023747, Train ACC:0.5371
Batch 230, Train Loss:0.023701, Train ACC:0.5378
Batch 231, Train Loss:0.023661, Train ACC:0.5384
Batch 232, Train Loss:0.023601, Train ACC:0.5395
Batch 233, Train Loss:0.023552, Train ACC:0.5403
Batch 234, Train Loss:0.023518, Train ACC:0.5407
Batch 235, Train Loss:0.023489, Train ACC:0.5410
Batch 236, Train Loss:0.023440, Train ACC:0.5418
Batch 237, Train Loss:0.023403, Train ACC:0.5423
Batch 238, Train Loss:0.023349, Train ACC:0.5433
Batch 239, Train Loss:0.023303, Train ACC:0.5441
Batch 240, Train Loss:0.023269, Train ACC:0.5444
Batch 241, Train Loss:0.023230, Train ACC:0.5450
Batch 242, Train Loss:0.023173, Train ACC:0.5460
Batch 243, Train Loss:0.023135, Train ACC:0.5465
Batch 244, Train Loss:0.023099, Train ACC:0.5470
Batch 245, Train Loss:0.023045, Train ACC:0.5480
Batch 246, Train Loss:0.023008, Train ACC:0.5485
Batch 247, Train Loss:0.022982, Train ACC:0.5487
Batch 248, Train Loss:0.022947, Train ACC:0.5492
Batch 249, Train Loss:0.022902, Train ACC:0.5499
Batch 250, Train Loss:0.022895, Train ACC:0.5496
Batch 251, Train Loss:0.022851, Train ACC:0.5503
Batch 252, Train Loss:0.022802, Train ACC:0.5512
Batch 253, Train Loss:0.022767, Train ACC:0.5516
Batch 254, Train Loss:0.022742, Train ACC:0.5518
Batch 255, Train Loss:0.022717, Train ACC:0.5520
Batch 256, Train Loss:0.022683, Train ACC:0.5525
Batch 257, Train Loss:0.022682, Train ACC:0.5520
Batch 258, Train Loss:0.022653, Train ACC:0.5523
Batch 259, Train Loss:0.022620, Train ACC:0.5528
Batch 260, Train Loss:0.022577, Train ACC:0.5535
Batch 261, Train Loss:0.022539, Train ACC:0.5540
Batch 262, Train Loss:0.022516, Train ACC:0.5542
Batch 263, Train Loss:0.022483, Train ACC:0.5546
Batch 264, Train Loss:0.022441, Train ACC:0.5553
Batch 265, Train Loss:0.022413, Train ACC:0.5556
Batch 266, Train Loss:0.022398, Train ACC:0.5555
Batch 267, Train Loss:0.022380, Train ACC:0.5556
Batch 268, Train Loss:0.022348, Train ACC:0.5560
Batch 269, Train Loss:0.022329, Train ACC:0.5560
Batch 270, Train Loss:0.022298, Train ACC:0.5564
Batch 271, Train Loss:0.022266, Train ACC:0.5568
Batch 272, Train Loss:0.022239, Train ACC:0.5571
Batch 273, Train Loss:0.022216, Train ACC:0.5573
Batch 274, Train Loss:0.022193, Train ACC:0.5574
Batch 275, Train Loss:0.022193, Train ACC:0.5570
Batch 276, Train Loss:0.022162, Train ACC:0.5574
Batch 277, Train Loss:0.022135, Train ACC:0.5576
Batch 278, Train Loss:0.022104, Train ACC:0.5580
Batch 279, Train Loss:0.022086, Train ACC:0.5581
Batch 280, Train Loss:0.022048, Train ACC:0.5587
Batch 281, Train Loss:0.022009, Train ACC:0.5593
Batch 282, Train Loss:0.021984, Train ACC:0.5596
Batch 283, Train Loss:0.021967, Train ACC:0.5596
Batch 284, Train Loss:0.021942, Train ACC:0.5599
Batch 285, Train Loss:0.021916, Train ACC:0.5601
Batch 286, Train Loss:0.021890, Train ACC:0.5604
Batch 287, Train Loss:0.021861, Train ACC:0.5607
Batch 288, Train Loss:0.021836, Train ACC:0.5610
Batch 289, Train Loss:0.021804, Train ACC:0.5615
Batch 290, Train Loss:0.021759, Train ACC:0.5623
Batch 291, Train Loss:0.021727, Train ACC:0.5628
Batch 292, Train Loss:0.021698, Train ACC:0.5631
Batch 293, Train Loss:0.021671, Train ACC:0.5635
Batch 294, Train Loss:0.021655, Train ACC:0.5635
Batch 295, Train Loss:0.021635, Train ACC:0.5636
Batch 296, Train Loss:0.021612, Train ACC:0.5639
Batch 297, Train Loss:0.021568, Train ACC:0.5646
Batch 298, Train Loss:0.021541, Train ACC:0.5650
Batch 299, Train Loss:0.021502, Train ACC:0.5657
Batch 300, Train Loss:0.021484, Train ACC:0.5658
预测标签：tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0], device='cuda:0'), 真实标签：tensor([0, 2, 2, 0, 0, 2, 2, 0, 0, 0, 2, 1, 0, 0, 0, 2, 0, 2, 1, 2, 2, 1, 1, 2,
        0, 1, 1, 2, 1, 2], device='cuda:0')
AlexNet(
  (features): Sequential(
    (0): Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (4): ReLU(inplace=True)
    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))
  (classifier): Sequential(
    (0): Linear(in_features=9216, out_features=4096, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=3, bias=True)
  )
)
Batch 1, Train Loss:0.036660, Train ACC:0.3667
Batch 2, Train Loss:0.044558, Train ACC:0.3667
Batch 3, Train Loss:0.041805, Train ACC:0.3778
Batch 4, Train Loss:0.040335, Train ACC:0.3750
Batch 5, Train Loss:0.039122, Train ACC:0.3933
Batch 6, Train Loss:0.037988, Train ACC:0.4000
Batch 7, Train Loss:0.037312, Train ACC:0.3762
Batch 8, Train Loss:0.036356, Train ACC:0.4125
Batch 9, Train Loss:0.036066, Train ACC:0.4333
Batch 10, Train Loss:0.034854, Train ACC:0.4533
Batch 11, Train Loss:0.034165, Train ACC:0.4697
Batch 12, Train Loss:0.032840, Train ACC:0.5139
Batch 13, Train Loss:0.031596, Train ACC:0.5282
Batch 14, Train Loss:0.030579, Train ACC:0.5333
Batch 15, Train Loss:0.029071, Train ACC:0.5644
Batch 16, Train Loss:0.027670, Train ACC:0.5917
Batch 17, Train Loss:0.026248, Train ACC:0.6157
Batch 18, Train Loss:0.024809, Train ACC:0.6370
Batch 19, Train Loss:0.023508, Train ACC:0.6561
Batch 20, Train Loss:0.022337, Train ACC:0.6733
Batch 21, Train Loss:0.021273, Train ACC:0.6889
Batch 22, Train Loss:0.020306, Train ACC:0.7030
Batch 23, Train Loss:0.019424, Train ACC:0.7159
Batch 24, Train Loss:0.018614, Train ACC:0.7278
Batch 25, Train Loss:0.018273, Train ACC:0.7360
Batch 26, Train Loss:0.071037, Train ACC:0.7256
Batch 27, Train Loss:0.068406, Train ACC:0.7358
Batch 28, Train Loss:0.069569, Train ACC:0.7357
Batch 29, Train Loss:0.077283, Train ACC:0.7322
Batch 30, Train Loss:0.075136, Train ACC:0.7389
Batch 31, Train Loss:0.072741, Train ACC:0.7473
Batch 32, Train Loss:0.070560, Train ACC:0.7531
Batch 33, Train Loss:0.068953, Train ACC:0.7576
Batch 34, Train Loss:0.068059, Train ACC:0.7569
Batch 35, Train Loss:0.066147, Train ACC:0.7638
Batch 36, Train Loss:0.064444, Train ACC:0.7685
Batch 37, Train Loss:0.063355, Train ACC:0.7658
Batch 38, Train Loss:0.061720, Train ACC:0.7719
Batch 39, Train Loss:0.060167, Train ACC:0.7778
Batch 40, Train Loss:0.058779, Train ACC:0.7817
Batch 41, Train Loss:0.057586, Train ACC:0.7837
Batch 42, Train Loss:0.056357, Train ACC:0.7873
Batch 43, Train Loss:0.055089, Train ACC:0.7922
Batch 44, Train Loss:0.053862, Train ACC:0.7970
Batch 45, Train Loss:0.052693, Train ACC:0.8015
Batch 46, Train Loss:0.051562, Train ACC:0.8058
Batch 47, Train Loss:0.050481, Train ACC:0.8099
Batch 48, Train Loss:0.049435, Train ACC:0.8139
Batch 49, Train Loss:0.048431, Train ACC:0.8177
Batch 50, Train Loss:0.047464, Train ACC:0.8213
Batch 51, Train Loss:0.046534, Train ACC:0.8248
Batch 52, Train Loss:0.045639, Train ACC:0.8282
Batch 53, Train Loss:0.044778, Train ACC:0.8314
Batch 54, Train Loss:0.043949, Train ACC:0.8346
Batch 55, Train Loss:0.043150, Train ACC:0.8376
Batch 56, Train Loss:0.042379, Train ACC:0.8405
Batch 57, Train Loss:0.041636, Train ACC:0.8433
Batch 58, Train Loss:0.040918, Train ACC:0.8460
Batch 59, Train Loss:0.040224, Train ACC:0.8486
Batch 60, Train Loss:0.039554, Train ACC:0.8511
Batch 61, Train Loss:0.038906, Train ACC:0.8536
Batch 62, Train Loss:0.038278, Train ACC:0.8559
Batch 63, Train Loss:0.037671, Train ACC:0.8582
Batch 64, Train Loss:0.037082, Train ACC:0.8604
Batch 65, Train Loss:0.036511, Train ACC:0.8626
Batch 66, Train Loss:0.035958, Train ACC:0.8646
Batch 67, Train Loss:0.035422, Train ACC:0.8667
Batch 68, Train Loss:0.034901, Train ACC:0.8686
Batch 69, Train Loss:0.034395, Train ACC:0.8705
Batch 70, Train Loss:0.033903, Train ACC:0.8724
Batch 71, Train Loss:0.033426, Train ACC:0.8742
Batch 72, Train Loss:0.032962, Train ACC:0.8759
Batch 73, Train Loss:0.032510, Train ACC:0.8776
Batch 74, Train Loss:0.032071, Train ACC:0.8793
Batch 75, Train Loss:0.031643, Train ACC:0.8809
Batch 76, Train Loss:0.031227, Train ACC:0.8825
Batch 77, Train Loss:0.030821, Train ACC:0.8840
Batch 78, Train Loss:0.030426, Train ACC:0.8855
Batch 79, Train Loss:0.030041, Train ACC:0.8869
Batch 80, Train Loss:0.029666, Train ACC:0.8883
Batch 81, Train Loss:0.029299, Train ACC:0.8897
Batch 82, Train Loss:0.028942, Train ACC:0.8911
Batch 83, Train Loss:0.028593, Train ACC:0.8924
Batch 84, Train Loss:0.028253, Train ACC:0.8937
Batch 85, Train Loss:0.027921, Train ACC:0.8949
Batch 86, Train Loss:0.027596, Train ACC:0.8961
Batch 87, Train Loss:0.027279, Train ACC:0.8973
Batch 88, Train Loss:0.026969, Train ACC:0.8985
Batch 89, Train Loss:0.026666, Train ACC:0.8996
Batch 90, Train Loss:0.026369, Train ACC:0.9007
Batch 91, Train Loss:0.026080, Train ACC:0.9018
Batch 92, Train Loss:0.025796, Train ACC:0.9029
Batch 93, Train Loss:0.025519, Train ACC:0.9039
Batch 94, Train Loss:0.025247, Train ACC:0.9050
Batch 95, Train Loss:0.024982, Train ACC:0.9060
Batch 96, Train Loss:0.024721, Train ACC:0.9069
Batch 97, Train Loss:0.024466, Train ACC:0.9079
Batch 98, Train Loss:0.024217, Train ACC:0.9088
Batch 99, Train Loss:0.023972, Train ACC:0.9098
Batch 100, Train Loss:0.023732, Train ACC:0.9107
Batch 101, Train Loss:0.023497, Train ACC:0.9116
Batch 102, Train Loss:0.023267, Train ACC:0.9124
Batch 103, Train Loss:0.023041, Train ACC:0.9133
Batch 104, Train Loss:0.022820, Train ACC:0.9141
Batch 105, Train Loss:0.022602, Train ACC:0.9149
Batch 106, Train Loss:0.022389, Train ACC:0.9157
Batch 107, Train Loss:0.022180, Train ACC:0.9165
Batch 108, Train Loss:0.021974, Train ACC:0.9173
Batch 109, Train Loss:0.021773, Train ACC:0.9180
Batch 110, Train Loss:0.021575, Train ACC:0.9188
Batch 111, Train Loss:0.021381, Train ACC:0.9195
Batch 112, Train Loss:0.021190, Train ACC:0.9202
Batch 113, Train Loss:0.021002, Train ACC:0.9209
Batch 114, Train Loss:0.020818, Train ACC:0.9216
Batch 115, Train Loss:0.020637, Train ACC:0.9223
Batch 116, Train Loss:0.020459, Train ACC:0.9230
Batch 117, Train Loss:0.020284, Train ACC:0.9236
Batch 118, Train Loss:0.020112, Train ACC:0.9243
Batch 119, Train Loss:0.019943, Train ACC:0.9249
Batch 120, Train Loss:0.019777, Train ACC:0.9256
Batch 121, Train Loss:0.019614, Train ACC:0.9262
Batch 122, Train Loss:0.019453, Train ACC:0.9268
Batch 123, Train Loss:0.019295, Train ACC:0.9274
Batch 124, Train Loss:0.019139, Train ACC:0.9280
Batch 125, Train Loss:0.018986, Train ACC:0.9285
Batch 126, Train Loss:0.018835, Train ACC:0.9291
Batch 127, Train Loss:0.018687, Train ACC:0.9297
Batch 128, Train Loss:0.018541, Train ACC:0.9302
Batch 129, Train Loss:0.018397, Train ACC:0.9307
Batch 130, Train Loss:0.018256, Train ACC:0.9313
Batch 131, Train Loss:0.018116, Train ACC:0.9318
Batch 132, Train Loss:0.017979, Train ACC:0.9323
Batch 133, Train Loss:0.017844, Train ACC:0.9328
Batch 134, Train Loss:0.017711, Train ACC:0.9333
Batch 135, Train Loss:0.017580, Train ACC:0.9338
Batch 136, Train Loss:0.017450, Train ACC:0.9343
Batch 137, Train Loss:0.017323, Train ACC:0.9348
Batch 138, Train Loss:0.017197, Train ACC:0.9353
Batch 139, Train Loss:0.017074, Train ACC:0.9357
Batch 140, Train Loss:0.016952, Train ACC:0.9362
Batch 141, Train Loss:0.016832, Train ACC:0.9366
Batch 142, Train Loss:0.016713, Train ACC:0.9371
Batch 143, Train Loss:0.016596, Train ACC:0.9375
Batch 144, Train Loss:0.016481, Train ACC:0.9380
Batch 145, Train Loss:0.016367, Train ACC:0.9384
Batch 146, Train Loss:0.016255, Train ACC:0.9388
Batch 147, Train Loss:0.016145, Train ACC:0.9392
Batch 148, Train Loss:0.016035, Train ACC:0.9396
Batch 149, Train Loss:0.015928, Train ACC:0.9400
Batch 150, Train Loss:0.015822, Train ACC:0.9404
Batch 151, Train Loss:0.015717, Train ACC:0.9408
Batch 152, Train Loss:0.015613, Train ACC:0.9412
Batch 153, Train Loss:0.015511, Train ACC:0.9416
Batch 154, Train Loss:0.015411, Train ACC:0.9420
Batch 155, Train Loss:0.015311, Train ACC:0.9424
Batch 156, Train Loss:0.015213, Train ACC:0.9427
Batch 157, Train Loss:0.015116, Train ACC:0.9431
Batch 158, Train Loss:0.015021, Train ACC:0.9435
Batch 159, Train Loss:0.014926, Train ACC:0.9438
Batch 160, Train Loss:0.014833, Train ACC:0.9442
Batch 161, Train Loss:0.014741, Train ACC:0.9445
Batch 162, Train Loss:0.014650, Train ACC:0.9449
Batch 163, Train Loss:0.014560, Train ACC:0.9452
Batch 164, Train Loss:0.014471, Train ACC:0.9455
Batch 165, Train Loss:0.014383, Train ACC:0.9459
Batch 166, Train Loss:0.014297, Train ACC:0.9462
Batch 167, Train Loss:0.014211, Train ACC:0.9465
Batch 168, Train Loss:0.014126, Train ACC:0.9468
Batch 169, Train Loss:0.014043, Train ACC:0.9471
Batch 170, Train Loss:0.013960, Train ACC:0.9475
Batch 171, Train Loss:0.013879, Train ACC:0.9478
Batch 172, Train Loss:0.013798, Train ACC:0.9481
Batch 173, Train Loss:0.013718, Train ACC:0.9484
Batch 174, Train Loss:0.013639, Train ACC:0.9487
Batch 175, Train Loss:0.013561, Train ACC:0.9490
Batch 176, Train Loss:0.013484, Train ACC:0.9492
Batch 177, Train Loss:0.013408, Train ACC:0.9495
Batch 178, Train Loss:0.013333, Train ACC:0.9498
Batch 179, Train Loss:0.013258, Train ACC:0.9501
Batch 180, Train Loss:0.013185, Train ACC:0.9504
Batch 181, Train Loss:0.013112, Train ACC:0.9506
Batch 182, Train Loss:0.013040, Train ACC:0.9509
Batch 183, Train Loss:0.012969, Train ACC:0.9512
Batch 184, Train Loss:0.012898, Train ACC:0.9514
Batch 185, Train Loss:0.012828, Train ACC:0.9517
Batch 186, Train Loss:0.012759, Train ACC:0.9520
Batch 187, Train Loss:0.012691, Train ACC:0.9522
Batch 188, Train Loss:0.012624, Train ACC:0.9525
Batch 189, Train Loss:0.012557, Train ACC:0.9527
Batch 190, Train Loss:0.012491, Train ACC:0.9530
Batch 191, Train Loss:0.012425, Train ACC:0.9532
Batch 192, Train Loss:0.012361, Train ACC:0.9535
Batch 193, Train Loss:0.012297, Train ACC:0.9537
Batch 194, Train Loss:0.012233, Train ACC:0.9540
Batch 195, Train Loss:0.012170, Train ACC:0.9542
Batch 196, Train Loss:0.012108, Train ACC:0.9544
Batch 197, Train Loss:0.012047, Train ACC:0.9547
Batch 198, Train Loss:0.011986, Train ACC:0.9549
Batch 199, Train Loss:0.011926, Train ACC:0.9551
Batch 200, Train Loss:0.011866, Train ACC:0.9553
Batch 201, Train Loss:0.011807, Train ACC:0.9556
Batch 202, Train Loss:0.011749, Train ACC:0.9558
Batch 203, Train Loss:0.011691, Train ACC:0.9560
Batch 204, Train Loss:0.011634, Train ACC:0.9562
Batch 205, Train Loss:0.011577, Train ACC:0.9564
Batch 206, Train Loss:0.011521, Train ACC:0.9566
Batch 207, Train Loss:0.011465, Train ACC:0.9568
Batch 208, Train Loss:0.011410, Train ACC:0.9571
Batch 209, Train Loss:0.011355, Train ACC:0.9573
Batch 210, Train Loss:0.011301, Train ACC:0.9575
Batch 211, Train Loss:0.011248, Train ACC:0.9577
Batch 212, Train Loss:0.011195, Train ACC:0.9579
Batch 213, Train Loss:0.011142, Train ACC:0.9581
Batch 214, Train Loss:0.011090, Train ACC:0.9583
Batch 215, Train Loss:0.011038, Train ACC:0.9584
Batch 216, Train Loss:0.010987, Train ACC:0.9586
Batch 217, Train Loss:0.010937, Train ACC:0.9588
Batch 218, Train Loss:0.010886, Train ACC:0.9590
Batch 219, Train Loss:0.010837, Train ACC:0.9592
Batch 220, Train Loss:0.010787, Train ACC:0.9594
Batch 221, Train Loss:0.010739, Train ACC:0.9596
Batch 222, Train Loss:0.010690, Train ACC:0.9598
Batch 223, Train Loss:0.010642, Train ACC:0.9599
Batch 224, Train Loss:0.010595, Train ACC:0.9601
Batch 225, Train Loss:0.010548, Train ACC:0.9603
Batch 226, Train Loss:0.010501, Train ACC:0.9605
Batch 227, Train Loss:0.010455, Train ACC:0.9606
Batch 228, Train Loss:0.010409, Train ACC:0.9608
Batch 229, Train Loss:0.010364, Train ACC:0.9610
Batch 230, Train Loss:0.010318, Train ACC:0.9612
Batch 231, Train Loss:0.010274, Train ACC:0.9613
Batch 232, Train Loss:0.010229, Train ACC:0.9615
Batch 233, Train Loss:0.010186, Train ACC:0.9617
Batch 234, Train Loss:0.010142, Train ACC:0.9618
Batch 235, Train Loss:0.010099, Train ACC:0.9620
Batch 236, Train Loss:0.010056, Train ACC:0.9621
Batch 237, Train Loss:0.010014, Train ACC:0.9623
Batch 238, Train Loss:0.009972, Train ACC:0.9625
Batch 239, Train Loss:0.009930, Train ACC:0.9626
Batch 240, Train Loss:0.009889, Train ACC:0.9628
Batch 241, Train Loss:0.009847, Train ACC:0.9629
Batch 242, Train Loss:0.009807, Train ACC:0.9631
Batch 243, Train Loss:0.009766, Train ACC:0.9632
Batch 244, Train Loss:0.009726, Train ACC:0.9634
Batch 245, Train Loss:0.009687, Train ACC:0.9635
Batch 246, Train Loss:0.009647, Train ACC:0.9637
Batch 247, Train Loss:0.009608, Train ACC:0.9638
Batch 248, Train Loss:0.009570, Train ACC:0.9640
Batch 249, Train Loss:0.009531, Train ACC:0.9641
Batch 250, Train Loss:0.009493, Train ACC:0.9643
Batch 251, Train Loss:0.009455, Train ACC:0.9644
Batch 252, Train Loss:0.009418, Train ACC:0.9646
Batch 253, Train Loss:0.009380, Train ACC:0.9647
Batch 254, Train Loss:0.009343, Train ACC:0.9648
Batch 255, Train Loss:0.009307, Train ACC:0.9650
Batch 256, Train Loss:0.009270, Train ACC:0.9651
Batch 257, Train Loss:0.009234, Train ACC:0.9652
Batch 258, Train Loss:0.009199, Train ACC:0.9654
Batch 259, Train Loss:0.009163, Train ACC:0.9655
Batch 260, Train Loss:0.009128, Train ACC:0.9656
Batch 261, Train Loss:0.009093, Train ACC:0.9658
Batch 262, Train Loss:0.009058, Train ACC:0.9659
Batch 263, Train Loss:0.009024, Train ACC:0.9660
Batch 264, Train Loss:0.008990, Train ACC:0.9662
Batch 265, Train Loss:0.008956, Train ACC:0.9663
Batch 266, Train Loss:0.008922, Train ACC:0.9664
Batch 267, Train Loss:0.008889, Train ACC:0.9665
Batch 268, Train Loss:0.008855, Train ACC:0.9667
Batch 269, Train Loss:0.008822, Train ACC:0.9668
Batch 270, Train Loss:0.008790, Train ACC:0.9669
Batch 271, Train Loss:0.008757, Train ACC:0.9670
Batch 272, Train Loss:0.008725, Train ACC:0.9672
Batch 273, Train Loss:0.008693, Train ACC:0.9673
Batch 274, Train Loss:0.008661, Train ACC:0.9674
Batch 275, Train Loss:0.008630, Train ACC:0.9675
Batch 276, Train Loss:0.008599, Train ACC:0.9676
Batch 277, Train Loss:0.008568, Train ACC:0.9677
Batch 278, Train Loss:0.008537, Train ACC:0.9679
Batch 279, Train Loss:0.008506, Train ACC:0.9680
Batch 280, Train Loss:0.008476, Train ACC:0.9681
Batch 281, Train Loss:0.008446, Train ACC:0.9682
Batch 282, Train Loss:0.008416, Train ACC:0.9683
Batch 283, Train Loss:0.008386, Train ACC:0.9684
Batch 284, Train Loss:0.008356, Train ACC:0.9685
Batch 285, Train Loss:0.008327, Train ACC:0.9687
Batch 286, Train Loss:0.008298, Train ACC:0.9688
Batch 287, Train Loss:0.008269, Train ACC:0.9689
Batch 288, Train Loss:0.008240, Train ACC:0.9690
Batch 289, Train Loss:0.008212, Train ACC:0.9691
Batch 290, Train Loss:0.008184, Train ACC:0.9692
Batch 291, Train Loss:0.008155, Train ACC:0.9693
Batch 292, Train Loss:0.008128, Train ACC:0.9694
Batch 293, Train Loss:0.008100, Train ACC:0.9695
Batch 294, Train Loss:0.008072, Train ACC:0.9696
Batch 295, Train Loss:0.008045, Train ACC:0.9697
Batch 296, Train Loss:0.008018, Train ACC:0.9698
Batch 297, Train Loss:0.007991, Train ACC:0.9699
Batch 298, Train Loss:0.007964, Train ACC:0.9700
Batch 299, Train Loss:0.007937, Train ACC:0.9701
Batch 300, Train Loss:0.007911, Train ACC:0.9702
预测标签：tensor([0, 1, 0, 2, 1, 0, 1, 2, 1, 1, 2, 0, 2, 2, 1, 1, 0, 0, 2, 0, 2, 0, 1, 0,
        0, 2, 1, 0, 0, 0], device='cuda:0'), 真实标签：tensor([0, 1, 0, 2, 1, 0, 1, 2, 1, 1, 2, 0, 2, 2, 1, 1, 0, 0, 2, 0, 2, 0, 1, 0,
        0, 2, 1, 0, 0, 0], device='cuda:0')
DenseNet(
  (features): Sequential(
    (conv0): Sequential(
      (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    )
    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu0): ReLU(inplace=True)
    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (denseblock1): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (transition1): _Transition(
      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (denseblock2): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (transition2): _Transition(
      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (denseblock3): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer13): _DenseLayer(
        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer14): _DenseLayer(
        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer15): _DenseLayer(
        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer16): _DenseLayer(
        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer17): _DenseLayer(
        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer18): _DenseLayer(
        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer19): _DenseLayer(
        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer20): _DenseLayer(
        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer21): _DenseLayer(
        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer22): _DenseLayer(
        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer23): _DenseLayer(
        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer24): _DenseLayer(
        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (transition3): _Transition(
      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (denseblock4): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer13): _DenseLayer(
        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer14): _DenseLayer(
        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer15): _DenseLayer(
        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer16): _DenseLayer(
        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=1024, out_features=3, bias=True)
  )
)
Batch 1, Train Loss:0.037438, Train ACC:0.3000
Batch 2, Train Loss:0.036110, Train ACC:0.3000
Batch 3, Train Loss:0.034597, Train ACC:0.3000
Batch 4, Train Loss:0.032998, Train ACC:0.4750
Batch 5, Train Loss:0.031597, Train ACC:0.5800
Batch 6, Train Loss:0.030415, Train ACC:0.6500
Batch 7, Train Loss:0.029666, Train ACC:0.7000
Batch 8, Train Loss:0.028594, Train ACC:0.7375
Batch 9, Train Loss:0.027551, Train ACC:0.7667
Batch 10, Train Loss:0.026487, Train ACC:0.7900
Batch 11, Train Loss:0.025752, Train ACC:0.8091
Batch 12, Train Loss:0.024833, Train ACC:0.8250
Batch 13, Train Loss:0.023928, Train ACC:0.8385
Batch 14, Train Loss:0.023152, Train ACC:0.8500
Batch 15, Train Loss:0.022958, Train ACC:0.8600
Batch 16, Train Loss:0.022170, Train ACC:0.8688
Batch 17, Train Loss:0.021628, Train ACC:0.8765
Batch 18, Train Loss:0.021014, Train ACC:0.8833
Batch 19, Train Loss:0.020476, Train ACC:0.8895
Batch 20, Train Loss:0.019851, Train ACC:0.8950
Batch 21, Train Loss:0.019263, Train ACC:0.9000
Batch 22, Train Loss:0.018785, Train ACC:0.9045
Batch 23, Train Loss:0.018264, Train ACC:0.9087
Batch 24, Train Loss:0.017918, Train ACC:0.9125
Batch 25, Train Loss:0.017420, Train ACC:0.9160
Batch 26, Train Loss:0.017014, Train ACC:0.9192
Batch 27, Train Loss:0.016549, Train ACC:0.9222
Batch 28, Train Loss:0.016104, Train ACC:0.9250
Batch 29, Train Loss:0.015708, Train ACC:0.9276
Batch 30, Train Loss:0.015311, Train ACC:0.9300
Batch 31, Train Loss:0.014955, Train ACC:0.9323
Batch 32, Train Loss:0.014617, Train ACC:0.9344
Batch 33, Train Loss:0.014293, Train ACC:0.9364
Batch 34, Train Loss:0.013956, Train ACC:0.9382
Batch 35, Train Loss:0.013625, Train ACC:0.9400
Batch 36, Train Loss:0.013325, Train ACC:0.9417
Batch 37, Train Loss:0.013039, Train ACC:0.9432
Batch 38, Train Loss:0.012751, Train ACC:0.9447
Batch 39, Train Loss:0.012477, Train ACC:0.9462
Batch 40, Train Loss:0.012245, Train ACC:0.9475
Batch 41, Train Loss:0.011984, Train ACC:0.9488
Batch 42, Train Loss:0.011749, Train ACC:0.9500
Batch 43, Train Loss:0.011521, Train ACC:0.9512
Batch 44, Train Loss:0.011290, Train ACC:0.9523
Batch 45, Train Loss:0.011068, Train ACC:0.9533
Batch 46, Train Loss:0.010897, Train ACC:0.9543
Batch 47, Train Loss:0.010823, Train ACC:0.9553
Batch 48, Train Loss:0.010637, Train ACC:0.9563
Batch 49, Train Loss:0.010451, Train ACC:0.9571
Batch 50, Train Loss:0.010267, Train ACC:0.9580
Batch 51, Train Loss:0.010132, Train ACC:0.9588
Batch 52, Train Loss:0.009967, Train ACC:0.9596
Batch 53, Train Loss:0.009802, Train ACC:0.9604
Batch 54, Train Loss:0.009660, Train ACC:0.9611
Batch 55, Train Loss:0.009508, Train ACC:0.9618
Batch 56, Train Loss:0.009358, Train ACC:0.9625
Batch 57, Train Loss:0.009223, Train ACC:0.9632
Batch 58, Train Loss:0.009083, Train ACC:0.9638
Batch 59, Train Loss:0.008984, Train ACC:0.9644
Batch 60, Train Loss:0.008876, Train ACC:0.9650
Batch 61, Train Loss:0.008748, Train ACC:0.9656
Batch 62, Train Loss:0.008632, Train ACC:0.9661
Batch 63, Train Loss:0.008510, Train ACC:0.9667
Batch 64, Train Loss:0.008392, Train ACC:0.9672
Batch 65, Train Loss:0.008292, Train ACC:0.9677
Batch 66, Train Loss:0.008180, Train ACC:0.9682
Batch 67, Train Loss:0.008109, Train ACC:0.9687
Batch 68, Train Loss:0.007999, Train ACC:0.9691
Batch 69, Train Loss:0.007894, Train ACC:0.9696
Batch 70, Train Loss:0.007792, Train ACC:0.9700
Batch 71, Train Loss:0.007708, Train ACC:0.9704
Batch 72, Train Loss:0.007634, Train ACC:0.9708
Batch 73, Train Loss:0.007544, Train ACC:0.9712
Batch 74, Train Loss:0.007455, Train ACC:0.9716
Batch 75, Train Loss:0.007366, Train ACC:0.9720
Batch 76, Train Loss:0.007279, Train ACC:0.9724
Batch 77, Train Loss:0.007195, Train ACC:0.9727
Batch 78, Train Loss:0.007111, Train ACC:0.9731
Batch 79, Train Loss:0.007036, Train ACC:0.9734
Batch 80, Train Loss:0.006955, Train ACC:0.9738
Batch 81, Train Loss:0.006876, Train ACC:0.9741
Batch 82, Train Loss:0.006798, Train ACC:0.9744
Batch 83, Train Loss:0.006740, Train ACC:0.9747
Batch 84, Train Loss:0.006665, Train ACC:0.9750
Batch 85, Train Loss:0.006593, Train ACC:0.9753
Batch 86, Train Loss:0.006526, Train ACC:0.9756
Batch 87, Train Loss:0.006459, Train ACC:0.9759
Batch 88, Train Loss:0.006396, Train ACC:0.9761
Batch 89, Train Loss:0.006334, Train ACC:0.9764
Batch 90, Train Loss:0.006277, Train ACC:0.9767
Batch 91, Train Loss:0.006213, Train ACC:0.9769
Batch 92, Train Loss:0.006154, Train ACC:0.9772
Batch 93, Train Loss:0.006096, Train ACC:0.9774
Batch 94, Train Loss:0.006038, Train ACC:0.9777
Batch 95, Train Loss:0.005979, Train ACC:0.9779
Batch 96, Train Loss:0.005924, Train ACC:0.9781
Batch 97, Train Loss:0.005868, Train ACC:0.9784
Batch 98, Train Loss:0.005812, Train ACC:0.9786
Batch 99, Train Loss:0.005758, Train ACC:0.9788
Batch 100, Train Loss:0.005706, Train ACC:0.9790
Batch 101, Train Loss:0.005653, Train ACC:0.9792
Batch 102, Train Loss:0.005602, Train ACC:0.9794
Batch 103, Train Loss:0.005551, Train ACC:0.9796
Batch 104, Train Loss:0.005501, Train ACC:0.9798
Batch 105, Train Loss:0.005451, Train ACC:0.9800
Batch 106, Train Loss:0.005413, Train ACC:0.9802
Batch 107, Train Loss:0.005365, Train ACC:0.9804
Batch 108, Train Loss:0.005318, Train ACC:0.9806
Batch 109, Train Loss:0.005273, Train ACC:0.9807
Batch 110, Train Loss:0.005229, Train ACC:0.9809
Batch 111, Train Loss:0.005188, Train ACC:0.9811
Batch 112, Train Loss:0.005147, Train ACC:0.9812
Batch 113, Train Loss:0.005104, Train ACC:0.9814
Batch 114, Train Loss:0.005062, Train ACC:0.9816
Batch 115, Train Loss:0.005022, Train ACC:0.9817
Batch 116, Train Loss:0.004981, Train ACC:0.9819
Batch 117, Train Loss:0.004940, Train ACC:0.9821
Batch 118, Train Loss:0.004902, Train ACC:0.9822
Batch 119, Train Loss:0.004868, Train ACC:0.9824
Batch 120, Train Loss:0.004831, Train ACC:0.9825
Batch 121, Train Loss:0.004798, Train ACC:0.9826
Batch 122, Train Loss:0.004761, Train ACC:0.9828
Batch 123, Train Loss:0.004725, Train ACC:0.9829
Batch 124, Train Loss:0.004694, Train ACC:0.9831
Batch 125, Train Loss:0.004658, Train ACC:0.9832
Batch 126, Train Loss:0.004627, Train ACC:0.9833
Batch 127, Train Loss:0.004594, Train ACC:0.9835
Batch 128, Train Loss:0.004561, Train ACC:0.9836
Batch 129, Train Loss:0.004529, Train ACC:0.9837
Batch 130, Train Loss:0.004498, Train ACC:0.9838
Batch 131, Train Loss:0.004465, Train ACC:0.9840
Batch 132, Train Loss:0.004434, Train ACC:0.9841
Batch 133, Train Loss:0.004402, Train ACC:0.9842
Batch 134, Train Loss:0.004372, Train ACC:0.9843
Batch 135, Train Loss:0.004343, Train ACC:0.9844
Batch 136, Train Loss:0.004314, Train ACC:0.9846
Batch 137, Train Loss:0.004286, Train ACC:0.9847
Batch 138, Train Loss:0.004265, Train ACC:0.9848
Batch 139, Train Loss:0.004237, Train ACC:0.9849
Batch 140, Train Loss:0.004210, Train ACC:0.9850
Batch 141, Train Loss:0.004182, Train ACC:0.9851
Batch 142, Train Loss:0.004155, Train ACC:0.9852
Batch 143, Train Loss:0.004134, Train ACC:0.9853
Batch 144, Train Loss:0.004107, Train ACC:0.9854
Batch 145, Train Loss:0.004080, Train ACC:0.9855
Batch 146, Train Loss:0.004059, Train ACC:0.9856
Batch 147, Train Loss:0.004033, Train ACC:0.9857
Batch 148, Train Loss:0.004007, Train ACC:0.9858
Batch 149, Train Loss:0.003982, Train ACC:0.9859
Batch 150, Train Loss:0.003957, Train ACC:0.9860
Batch 151, Train Loss:0.003933, Train ACC:0.9861
Batch 152, Train Loss:0.003910, Train ACC:0.9862
Batch 153, Train Loss:0.003886, Train ACC:0.9863
Batch 154, Train Loss:0.003863, Train ACC:0.9864
Batch 155, Train Loss:0.003840, Train ACC:0.9865
Batch 156, Train Loss:0.003817, Train ACC:0.9865
Batch 157, Train Loss:0.003794, Train ACC:0.9866
Batch 158, Train Loss:0.003772, Train ACC:0.9867
Batch 159, Train Loss:0.003751, Train ACC:0.9868
Batch 160, Train Loss:0.003729, Train ACC:0.9869
Batch 161, Train Loss:0.003707, Train ACC:0.9870
Batch 162, Train Loss:0.003687, Train ACC:0.9870
Batch 163, Train Loss:0.003668, Train ACC:0.9871
Batch 164, Train Loss:0.003647, Train ACC:0.9872
Batch 165, Train Loss:0.003633, Train ACC:0.9873
Batch 166, Train Loss:0.003613, Train ACC:0.9873
Batch 167, Train Loss:0.003593, Train ACC:0.9874
Batch 168, Train Loss:0.003573, Train ACC:0.9875
Batch 169, Train Loss:0.003554, Train ACC:0.9876
Batch 170, Train Loss:0.003535, Train ACC:0.9876
Batch 171, Train Loss:0.003515, Train ACC:0.9877
Batch 172, Train Loss:0.003496, Train ACC:0.9878
Batch 173, Train Loss:0.003478, Train ACC:0.9879
Batch 174, Train Loss:0.003459, Train ACC:0.9879
Batch 175, Train Loss:0.003440, Train ACC:0.9880
Batch 176, Train Loss:0.003423, Train ACC:0.9881
Batch 177, Train Loss:0.003406, Train ACC:0.9881
Batch 178, Train Loss:0.003389, Train ACC:0.9882
Batch 179, Train Loss:0.003373, Train ACC:0.9883
Batch 180, Train Loss:0.003357, Train ACC:0.9883
Batch 181, Train Loss:0.003340, Train ACC:0.9884
Batch 182, Train Loss:0.003322, Train ACC:0.9885
Batch 183, Train Loss:0.003309, Train ACC:0.9885
Batch 184, Train Loss:0.003292, Train ACC:0.9886
Batch 185, Train Loss:0.003276, Train ACC:0.9886
Batch 186, Train Loss:0.003259, Train ACC:0.9887
Batch 187, Train Loss:0.003243, Train ACC:0.9888
Batch 188, Train Loss:0.003227, Train ACC:0.9888
Batch 189, Train Loss:0.003211, Train ACC:0.9889
Batch 190, Train Loss:0.003196, Train ACC:0.9889
Batch 191, Train Loss:0.003180, Train ACC:0.9890
Batch 192, Train Loss:0.003165, Train ACC:0.9891
Batch 193, Train Loss:0.003149, Train ACC:0.9891
Batch 194, Train Loss:0.003136, Train ACC:0.9892
Batch 195, Train Loss:0.003121, Train ACC:0.9892
Batch 196, Train Loss:0.003106, Train ACC:0.9893
Batch 197, Train Loss:0.003091, Train ACC:0.9893
Batch 198, Train Loss:0.003077, Train ACC:0.9894
Batch 199, Train Loss:0.003063, Train ACC:0.9894
Batch 200, Train Loss:0.003049, Train ACC:0.9895
Batch 201, Train Loss:0.003035, Train ACC:0.9896
Batch 202, Train Loss:0.003021, Train ACC:0.9896
Batch 203, Train Loss:0.003007, Train ACC:0.9897
Batch 204, Train Loss:0.002993, Train ACC:0.9897
Batch 205, Train Loss:0.002980, Train ACC:0.9898
Batch 206, Train Loss:0.002967, Train ACC:0.9898
Batch 207, Train Loss:0.002953, Train ACC:0.9899
Batch 208, Train Loss:0.002940, Train ACC:0.9899
Batch 209, Train Loss:0.002933, Train ACC:0.9900
Batch 210, Train Loss:0.002919, Train ACC:0.9900
Batch 211, Train Loss:0.002906, Train ACC:0.9900
Batch 212, Train Loss:0.002893, Train ACC:0.9901
Batch 213, Train Loss:0.002880, Train ACC:0.9901
Batch 214, Train Loss:0.002867, Train ACC:0.9902
Batch 215, Train Loss:0.002854, Train ACC:0.9902
Batch 216, Train Loss:0.002842, Train ACC:0.9903
Batch 217, Train Loss:0.002830, Train ACC:0.9903
Batch 218, Train Loss:0.002817, Train ACC:0.9904
Batch 219, Train Loss:0.002806, Train ACC:0.9904
Batch 220, Train Loss:0.002794, Train ACC:0.9905
Batch 221, Train Loss:0.002782, Train ACC:0.9905
Batch 222, Train Loss:0.002770, Train ACC:0.9905
Batch 223, Train Loss:0.002758, Train ACC:0.9906
Batch 224, Train Loss:0.002748, Train ACC:0.9906
Batch 225, Train Loss:0.002736, Train ACC:0.9907
Batch 226, Train Loss:0.002725, Train ACC:0.9907
Batch 227, Train Loss:0.002713, Train ACC:0.9907
Batch 228, Train Loss:0.002702, Train ACC:0.9908
Batch 229, Train Loss:0.002690, Train ACC:0.9908
Batch 230, Train Loss:0.002680, Train ACC:0.9909
Batch 231, Train Loss:0.002669, Train ACC:0.9909
Batch 232, Train Loss:0.002658, Train ACC:0.9909
Batch 233, Train Loss:0.002647, Train ACC:0.9910
Batch 234, Train Loss:0.002637, Train ACC:0.9910
Batch 235, Train Loss:0.002626, Train ACC:0.9911
Batch 236, Train Loss:0.002616, Train ACC:0.9911
Batch 237, Train Loss:0.002605, Train ACC:0.9911
Batch 238, Train Loss:0.002595, Train ACC:0.9912
Batch 239, Train Loss:0.002584, Train ACC:0.9912
Batch 240, Train Loss:0.002574, Train ACC:0.9912
Batch 241, Train Loss:0.002564, Train ACC:0.9913
Batch 242, Train Loss:0.002554, Train ACC:0.9913
Batch 243, Train Loss:0.002544, Train ACC:0.9914
Batch 244, Train Loss:0.002538, Train ACC:0.9914
Batch 245, Train Loss:0.002528, Train ACC:0.9914
Batch 246, Train Loss:0.002519, Train ACC:0.9915
Batch 247, Train Loss:0.002509, Train ACC:0.9915
Batch 248, Train Loss:0.002499, Train ACC:0.9915
Batch 249, Train Loss:0.002489, Train ACC:0.9916
Batch 250, Train Loss:0.002480, Train ACC:0.9916
Batch 251, Train Loss:0.002470, Train ACC:0.9916
Batch 252, Train Loss:0.002461, Train ACC:0.9917
Batch 253, Train Loss:0.002452, Train ACC:0.9917
Batch 254, Train Loss:0.002443, Train ACC:0.9917
Batch 255, Train Loss:0.002434, Train ACC:0.9918
Batch 256, Train Loss:0.002426, Train ACC:0.9918
Batch 257, Train Loss:0.002420, Train ACC:0.9918
Batch 258, Train Loss:0.002411, Train ACC:0.9919
Batch 259, Train Loss:0.002402, Train ACC:0.9919
Batch 260, Train Loss:0.002393, Train ACC:0.9919
Batch 261, Train Loss:0.002388, Train ACC:0.9920
Batch 262, Train Loss:0.002380, Train ACC:0.9920
Batch 263, Train Loss:0.002373, Train ACC:0.9920
Batch 264, Train Loss:0.002365, Train ACC:0.9920
Batch 265, Train Loss:0.002356, Train ACC:0.9921
Batch 266, Train Loss:0.002348, Train ACC:0.9921
Batch 267, Train Loss:0.002339, Train ACC:0.9921
Batch 268, Train Loss:0.002331, Train ACC:0.9922
Batch 269, Train Loss:0.002323, Train ACC:0.9922
Batch 270, Train Loss:0.002315, Train ACC:0.9922
Batch 271, Train Loss:0.002307, Train ACC:0.9923
Batch 272, Train Loss:0.002299, Train ACC:0.9923
Batch 273, Train Loss:0.002291, Train ACC:0.9923
Batch 274, Train Loss:0.002283, Train ACC:0.9923
Batch 275, Train Loss:0.002275, Train ACC:0.9924
Batch 276, Train Loss:0.002267, Train ACC:0.9924
Batch 277, Train Loss:0.002260, Train ACC:0.9924
Batch 278, Train Loss:0.002252, Train ACC:0.9924
Batch 279, Train Loss:0.002244, Train ACC:0.9925
Batch 280, Train Loss:0.002236, Train ACC:0.9925
Batch 281, Train Loss:0.002229, Train ACC:0.9925
Batch 282, Train Loss:0.002221, Train ACC:0.9926
Batch 283, Train Loss:0.002214, Train ACC:0.9926
Batch 284, Train Loss:0.002206, Train ACC:0.9926
Batch 285, Train Loss:0.002199, Train ACC:0.9926
Batch 286, Train Loss:0.002192, Train ACC:0.9927
Batch 287, Train Loss:0.002185, Train ACC:0.9927
Batch 288, Train Loss:0.002177, Train ACC:0.9927
Batch 289, Train Loss:0.002170, Train ACC:0.9927
Batch 290, Train Loss:0.002164, Train ACC:0.9928
Batch 291, Train Loss:0.002157, Train ACC:0.9928
Batch 292, Train Loss:0.002150, Train ACC:0.9928
Batch 293, Train Loss:0.002143, Train ACC:0.9928
Batch 294, Train Loss:0.002136, Train ACC:0.9929
Batch 295, Train Loss:0.002129, Train ACC:0.9929
Batch 296, Train Loss:0.002122, Train ACC:0.9929
Batch 297, Train Loss:0.002115, Train ACC:0.9929
Batch 298, Train Loss:0.002109, Train ACC:0.9930
Batch 299, Train Loss:0.002102, Train ACC:0.9930
Batch 300, Train Loss:0.002095, Train ACC:0.9930
预测标签：tensor([2, 0, 1, 1, 2, 2, 0, 2, 0, 0, 0, 2, 1, 2, 1, 0, 2, 1, 2, 1, 0, 1, 0, 2,
        1, 2, 1, 2, 0, 0], device='cuda:0'), 真实标签：tensor([2, 0, 1, 1, 2, 2, 0, 2, 0, 0, 0, 2, 1, 2, 1, 0, 2, 1, 2, 1, 0, 1, 0, 2,
        1, 2, 1, 2, 0, 0], device='cuda:0')
GoogLeNet(
  (conv1): Sequential(
    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
  )
  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
  (conv2): BasicConv2d(
    (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
  )
  (conv3): BasicConv2d(
    (conv): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
  )
  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
  (inception3a): Inception(
    (branch1): BasicConv2d(
      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch2): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch4): Sequential(
      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
      (1): BasicConv2d(
        (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (inception3b): Inception(
    (branch1): BasicConv2d(
      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch2): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch4): Sequential(
      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
      (1): BasicConv2d(
        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (maxpool3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
  (inception4a): Inception(
    (branch1): BasicConv2d(
      (conv): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch2): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(208, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch4): Sequential(
      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
      (1): BasicConv2d(
        (conv): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (inception4b): Inception(
    (branch1): BasicConv2d(
      (conv): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch2): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch4): Sequential(
      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
      (1): BasicConv2d(
        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (inception4c): Inception(
    (branch1): BasicConv2d(
      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch2): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch4): Sequential(
      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
      (1): BasicConv2d(
        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (inception4d): Inception(
    (branch1): BasicConv2d(
      (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch2): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch4): Sequential(
      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
      (1): BasicConv2d(
        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (inception4e): Inception(
    (branch1): BasicConv2d(
      (conv): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch2): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch4): Sequential(
      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
      (1): BasicConv2d(
        (conv): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)
  (inception5a): Inception(
    (branch1): BasicConv2d(
      (conv): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch2): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch4): Sequential(
      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
      (1): BasicConv2d(
        (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (inception5b): Inception(
    (branch1): BasicConv2d(
      (conv): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch2): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch4): Sequential(
      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
      (1): BasicConv2d(
        (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (aux1): InceptionAux(
    (conv): BasicConv2d(
      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (fc1): Linear(in_features=2048, out_features=1024, bias=True)
    (fc2): Linear(in_features=1024, out_features=3, bias=True)
  )
  (aux2): InceptionAux(
    (conv): BasicConv2d(
      (conv): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (fc1): Linear(in_features=2048, out_features=1024, bias=True)
    (fc2): Linear(in_features=1024, out_features=3, bias=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.2, inplace=False)
  (fc): Linear(in_features=1024, out_features=3, bias=True)
)
Batch 1, Train Loss:0.037057, Train ACC:0.2333
Batch 2, Train Loss:0.036280, Train ACC:0.3833
Batch 3, Train Loss:0.034433, Train ACC:0.5000
Batch 4, Train Loss:0.034756, Train ACC:0.5167
Batch 5, Train Loss:0.034218, Train ACC:0.5267
Batch 6, Train Loss:0.033073, Train ACC:0.5722
Batch 7, Train Loss:0.031831, Train ACC:0.6333
Batch 8, Train Loss:0.030519, Train ACC:0.6792
Batch 9, Train Loss:0.029709, Train ACC:0.7111
Batch 10, Train Loss:0.028680, Train ACC:0.7400
Batch 11, Train Loss:0.027678, Train ACC:0.7636
Batch 12, Train Loss:0.026985, Train ACC:0.7833
Batch 13, Train Loss:0.026593, Train ACC:0.8000
Batch 14, Train Loss:0.025906, Train ACC:0.8143
Batch 15, Train Loss:0.025184, Train ACC:0.8267
Batch 16, Train Loss:0.024405, Train ACC:0.8375
Batch 17, Train Loss:0.023680, Train ACC:0.8471
Batch 18, Train Loss:0.022979, Train ACC:0.8556
Batch 19, Train Loss:0.022422, Train ACC:0.8632
Batch 20, Train Loss:0.021707, Train ACC:0.8700
Batch 21, Train Loss:0.021165, Train ACC:0.8762
Batch 22, Train Loss:0.020523, Train ACC:0.8818
Batch 23, Train Loss:0.020032, Train ACC:0.8870
Batch 24, Train Loss:0.019429, Train ACC:0.8917
Batch 25, Train Loss:0.018978, Train ACC:0.8960
Batch 26, Train Loss:0.018445, Train ACC:0.9000
Batch 27, Train Loss:0.018035, Train ACC:0.9037
Batch 28, Train Loss:0.017645, Train ACC:0.9071
Batch 29, Train Loss:0.017187, Train ACC:0.9103
Batch 30, Train Loss:0.016749, Train ACC:0.9133
Batch 31, Train Loss:0.016322, Train ACC:0.9161
Batch 32, Train Loss:0.015976, Train ACC:0.9187
Batch 33, Train Loss:0.015621, Train ACC:0.9212
Batch 34, Train Loss:0.015297, Train ACC:0.9235
Batch 35, Train Loss:0.014928, Train ACC:0.9257
Batch 36, Train Loss:0.014609, Train ACC:0.9278
Batch 37, Train Loss:0.014322, Train ACC:0.9297
Batch 38, Train Loss:0.014004, Train ACC:0.9316
Batch 39, Train Loss:0.013695, Train ACC:0.9333
Batch 40, Train Loss:0.013400, Train ACC:0.9350
Batch 41, Train Loss:0.013137, Train ACC:0.9366
Batch 42, Train Loss:0.013029, Train ACC:0.9381
Batch 43, Train Loss:0.012797, Train ACC:0.9395
Batch 44, Train Loss:0.012647, Train ACC:0.9409
Batch 45, Train Loss:0.012399, Train ACC:0.9422
Batch 46, Train Loss:0.012161, Train ACC:0.9435
Batch 47, Train Loss:0.011946, Train ACC:0.9447
Batch 48, Train Loss:0.011730, Train ACC:0.9458
Batch 49, Train Loss:0.011538, Train ACC:0.9469
Batch 50, Train Loss:0.011368, Train ACC:0.9480
Batch 51, Train Loss:0.011181, Train ACC:0.9490
Batch 52, Train Loss:0.011008, Train ACC:0.9500
Batch 53, Train Loss:0.010859, Train ACC:0.9509
Batch 54, Train Loss:0.010703, Train ACC:0.9519
Batch 55, Train Loss:0.010548, Train ACC:0.9527
Batch 56, Train Loss:0.010376, Train ACC:0.9536
Batch 57, Train Loss:0.010211, Train ACC:0.9544
Batch 58, Train Loss:0.010120, Train ACC:0.9552
Batch 59, Train Loss:0.009962, Train ACC:0.9559
Batch 60, Train Loss:0.009816, Train ACC:0.9567
Batch 61, Train Loss:0.009666, Train ACC:0.9574
Batch 62, Train Loss:0.009527, Train ACC:0.9581
Batch 63, Train Loss:0.009405, Train ACC:0.9587
Batch 64, Train Loss:0.009271, Train ACC:0.9594
Batch 65, Train Loss:0.009154, Train ACC:0.9600
Batch 66, Train Loss:0.009067, Train ACC:0.9606
Batch 67, Train Loss:0.008941, Train ACC:0.9612
Batch 68, Train Loss:0.008835, Train ACC:0.9618
Batch 69, Train Loss:0.008719, Train ACC:0.9623
Batch 70, Train Loss:0.008601, Train ACC:0.9629
Batch 71, Train Loss:0.008487, Train ACC:0.9634
Batch 72, Train Loss:0.008378, Train ACC:0.9639
Batch 73, Train Loss:0.008269, Train ACC:0.9644
Batch 74, Train Loss:0.008166, Train ACC:0.9649
Batch 75, Train Loss:0.008064, Train ACC:0.9653
Batch 76, Train Loss:0.007964, Train ACC:0.9658
Batch 77, Train Loss:0.007868, Train ACC:0.9662
Batch 78, Train Loss:0.007772, Train ACC:0.9667
Batch 79, Train Loss:0.007681, Train ACC:0.9671
Batch 80, Train Loss:0.007604, Train ACC:0.9675
Batch 81, Train Loss:0.007519, Train ACC:0.9679
Batch 82, Train Loss:0.007433, Train ACC:0.9683
Batch 83, Train Loss:0.007357, Train ACC:0.9687
Batch 84, Train Loss:0.007275, Train ACC:0.9690
Batch 85, Train Loss:0.007200, Train ACC:0.9694
Batch 86, Train Loss:0.007123, Train ACC:0.9698
Batch 87, Train Loss:0.007049, Train ACC:0.9701
Batch 88, Train Loss:0.006972, Train ACC:0.9705
Batch 89, Train Loss:0.006901, Train ACC:0.9708
Batch 90, Train Loss:0.006827, Train ACC:0.9711
Batch 91, Train Loss:0.006757, Train ACC:0.9714
Batch 92, Train Loss:0.006707, Train ACC:0.9717
Batch 93, Train Loss:0.006644, Train ACC:0.9720
Batch 94, Train Loss:0.006590, Train ACC:0.9723
Batch 95, Train Loss:0.006523, Train ACC:0.9726
Batch 96, Train Loss:0.006486, Train ACC:0.9729
Batch 97, Train Loss:0.006444, Train ACC:0.9732
Batch 98, Train Loss:0.006389, Train ACC:0.9735
Batch 99, Train Loss:0.006328, Train ACC:0.9737
Batch 100, Train Loss:0.006267, Train ACC:0.9740
Batch 101, Train Loss:0.006214, Train ACC:0.9743
Batch 102, Train Loss:0.006160, Train ACC:0.9745
Batch 103, Train Loss:0.006105, Train ACC:0.9748
Batch 104, Train Loss:0.006049, Train ACC:0.9750
Batch 105, Train Loss:0.005994, Train ACC:0.9752
Batch 106, Train Loss:0.005940, Train ACC:0.9755
Batch 107, Train Loss:0.005887, Train ACC:0.9757
Batch 108, Train Loss:0.005858, Train ACC:0.9759
Batch 109, Train Loss:0.005810, Train ACC:0.9761
Batch 110, Train Loss:0.005761, Train ACC:0.9764
Batch 111, Train Loss:0.005714, Train ACC:0.9766
Batch 112, Train Loss:0.005667, Train ACC:0.9768
Batch 113, Train Loss:0.005619, Train ACC:0.9770
Batch 114, Train Loss:0.005573, Train ACC:0.9772
Batch 115, Train Loss:0.005550, Train ACC:0.9774
Batch 116, Train Loss:0.005505, Train ACC:0.9776
Batch 117, Train Loss:0.005462, Train ACC:0.9778
Batch 118, Train Loss:0.005418, Train ACC:0.9780
Batch 119, Train Loss:0.005377, Train ACC:0.9782
Batch 120, Train Loss:0.005340, Train ACC:0.9783
Batch 121, Train Loss:0.005299, Train ACC:0.9785
Batch 122, Train Loss:0.005260, Train ACC:0.9787
Batch 123, Train Loss:0.005225, Train ACC:0.9789
Batch 124, Train Loss:0.005192, Train ACC:0.9790
Batch 125, Train Loss:0.005153, Train ACC:0.9792
Batch 126, Train Loss:0.005129, Train ACC:0.9794
Batch 127, Train Loss:0.005090, Train ACC:0.9795
Batch 128, Train Loss:0.005054, Train ACC:0.9797
Batch 129, Train Loss:0.005016, Train ACC:0.9798
Batch 130, Train Loss:0.004980, Train ACC:0.9800
Batch 131, Train Loss:0.004943, Train ACC:0.9802
Batch 132, Train Loss:0.004908, Train ACC:0.9803
Batch 133, Train Loss:0.004872, Train ACC:0.9805
Batch 134, Train Loss:0.004841, Train ACC:0.9806
Batch 135, Train Loss:0.004808, Train ACC:0.9807
Batch 136, Train Loss:0.004779, Train ACC:0.9809
Batch 137, Train Loss:0.004745, Train ACC:0.9810
Batch 138, Train Loss:0.004719, Train ACC:0.9812
Batch 139, Train Loss:0.004686, Train ACC:0.9813
Batch 140, Train Loss:0.004654, Train ACC:0.9814
Batch 141, Train Loss:0.004625, Train ACC:0.9816
Batch 142, Train Loss:0.004594, Train ACC:0.9817
Batch 143, Train Loss:0.004564, Train ACC:0.9818
Batch 144, Train Loss:0.004535, Train ACC:0.9819
Batch 145, Train Loss:0.004506, Train ACC:0.9821
Batch 146, Train Loss:0.004478, Train ACC:0.9822
Batch 147, Train Loss:0.004452, Train ACC:0.9823
Batch 148, Train Loss:0.004424, Train ACC:0.9824
Batch 149, Train Loss:0.004396, Train ACC:0.9826
Batch 150, Train Loss:0.004381, Train ACC:0.9827
Batch 151, Train Loss:0.004354, Train ACC:0.9828
Batch 152, Train Loss:0.004327, Train ACC:0.9829
Batch 153, Train Loss:0.004301, Train ACC:0.9830
Batch 154, Train Loss:0.004274, Train ACC:0.9831
Batch 155, Train Loss:0.004249, Train ACC:0.9832
Batch 156, Train Loss:0.004223, Train ACC:0.9833
Batch 157, Train Loss:0.004197, Train ACC:0.9834
Batch 158, Train Loss:0.004179, Train ACC:0.9835
Batch 159, Train Loss:0.004161, Train ACC:0.9836
Batch 160, Train Loss:0.004136, Train ACC:0.9838
Batch 161, Train Loss:0.004112, Train ACC:0.9839
Batch 162, Train Loss:0.004088, Train ACC:0.9840
Batch 163, Train Loss:0.004064, Train ACC:0.9840
Batch 164, Train Loss:0.004040, Train ACC:0.9841
Batch 165, Train Loss:0.004018, Train ACC:0.9842
Batch 166, Train Loss:0.003994, Train ACC:0.9843
Batch 167, Train Loss:0.003972, Train ACC:0.9844
Batch 168, Train Loss:0.003949, Train ACC:0.9845
Batch 169, Train Loss:0.003928, Train ACC:0.9846
Batch 170, Train Loss:0.003907, Train ACC:0.9847
Batch 171, Train Loss:0.003885, Train ACC:0.9848
Batch 172, Train Loss:0.003864, Train ACC:0.9849
Batch 173, Train Loss:0.003843, Train ACC:0.9850
Batch 174, Train Loss:0.003824, Train ACC:0.9851
Batch 175, Train Loss:0.003806, Train ACC:0.9851
Batch 176, Train Loss:0.003785, Train ACC:0.9852
Batch 177, Train Loss:0.003764, Train ACC:0.9853
Batch 178, Train Loss:0.003744, Train ACC:0.9854
Batch 179, Train Loss:0.003723, Train ACC:0.9855
Batch 180, Train Loss:0.003703, Train ACC:0.9856
Batch 181, Train Loss:0.003684, Train ACC:0.9856
Batch 182, Train Loss:0.003665, Train ACC:0.9857
Batch 183, Train Loss:0.003646, Train ACC:0.9858
Batch 184, Train Loss:0.003627, Train ACC:0.9859
Batch 185, Train Loss:0.003609, Train ACC:0.9859
Batch 186, Train Loss:0.003591, Train ACC:0.9860
Batch 187, Train Loss:0.003572, Train ACC:0.9861
Batch 188, Train Loss:0.003554, Train ACC:0.9862
Batch 189, Train Loss:0.003536, Train ACC:0.9862
Batch 190, Train Loss:0.003518, Train ACC:0.9863
Batch 191, Train Loss:0.003500, Train ACC:0.9864
Batch 192, Train Loss:0.003482, Train ACC:0.9865
Batch 193, Train Loss:0.003465, Train ACC:0.9865
Batch 194, Train Loss:0.003447, Train ACC:0.9866
Batch 195, Train Loss:0.003432, Train ACC:0.9867
Batch 196, Train Loss:0.003416, Train ACC:0.9867
Batch 197, Train Loss:0.003399, Train ACC:0.9868
Batch 198, Train Loss:0.003383, Train ACC:0.9869
Batch 199, Train Loss:0.003366, Train ACC:0.9869
Batch 200, Train Loss:0.003350, Train ACC:0.9870
Batch 201, Train Loss:0.003335, Train ACC:0.9871
Batch 202, Train Loss:0.003318, Train ACC:0.9871
Batch 203, Train Loss:0.003302, Train ACC:0.9872
Batch 204, Train Loss:0.003287, Train ACC:0.9873
Batch 205, Train Loss:0.003273, Train ACC:0.9873
Batch 206, Train Loss:0.003257, Train ACC:0.9874
Batch 207, Train Loss:0.003242, Train ACC:0.9874
Batch 208, Train Loss:0.003226, Train ACC:0.9875
Batch 209, Train Loss:0.003211, Train ACC:0.9876
Batch 210, Train Loss:0.003196, Train ACC:0.9876
Batch 211, Train Loss:0.003182, Train ACC:0.9877
Batch 212, Train Loss:0.003167, Train ACC:0.9877
Batch 213, Train Loss:0.003153, Train ACC:0.9878
Batch 214, Train Loss:0.003138, Train ACC:0.9879
Batch 215, Train Loss:0.003124, Train ACC:0.9879
Batch 216, Train Loss:0.003110, Train ACC:0.9880
Batch 217, Train Loss:0.003096, Train ACC:0.9880
Batch 218, Train Loss:0.003083, Train ACC:0.9881
Batch 219, Train Loss:0.003069, Train ACC:0.9881
Batch 220, Train Loss:0.003055, Train ACC:0.9882
Batch 221, Train Loss:0.003042, Train ACC:0.9882
Batch 222, Train Loss:0.003029, Train ACC:0.9883
Batch 223, Train Loss:0.003016, Train ACC:0.9883
Batch 224, Train Loss:0.003003, Train ACC:0.9884
Batch 225, Train Loss:0.002990, Train ACC:0.9884
Batch 226, Train Loss:0.002977, Train ACC:0.9885
Batch 227, Train Loss:0.002965, Train ACC:0.9885
Batch 228, Train Loss:0.002952, Train ACC:0.9886
Batch 229, Train Loss:0.002939, Train ACC:0.9886
Batch 230, Train Loss:0.002927, Train ACC:0.9887
Batch 231, Train Loss:0.002914, Train ACC:0.9887
Batch 232, Train Loss:0.002902, Train ACC:0.9888
Batch 233, Train Loss:0.002891, Train ACC:0.9888
Batch 234, Train Loss:0.002879, Train ACC:0.9889
Batch 235, Train Loss:0.002867, Train ACC:0.9889
Batch 236, Train Loss:0.002855, Train ACC:0.9890
Batch 237, Train Loss:0.002844, Train ACC:0.9890
Batch 238, Train Loss:0.002832, Train ACC:0.9891
Batch 239, Train Loss:0.002821, Train ACC:0.9891
Batch 240, Train Loss:0.002809, Train ACC:0.9892
Batch 241, Train Loss:0.002799, Train ACC:0.9892
Batch 242, Train Loss:0.002787, Train ACC:0.9893
Batch 243, Train Loss:0.002776, Train ACC:0.9893
Batch 244, Train Loss:0.002765, Train ACC:0.9893
Batch 245, Train Loss:0.002754, Train ACC:0.9894
Batch 246, Train Loss:0.002746, Train ACC:0.9894
Batch 247, Train Loss:0.002738, Train ACC:0.9895
Batch 248, Train Loss:0.002727, Train ACC:0.9895
Batch 249, Train Loss:0.002718, Train ACC:0.9896
Batch 250, Train Loss:0.002708, Train ACC:0.9896
Batch 251, Train Loss:0.002697, Train ACC:0.9896
Batch 252, Train Loss:0.002687, Train ACC:0.9897
Batch 253, Train Loss:0.002677, Train ACC:0.9897
Batch 254, Train Loss:0.002666, Train ACC:0.9898
Batch 255, Train Loss:0.002656, Train ACC:0.9898
Batch 256, Train Loss:0.002647, Train ACC:0.9898
Batch 257, Train Loss:0.002637, Train ACC:0.9899
Batch 258, Train Loss:0.002627, Train ACC:0.9899
Batch 259, Train Loss:0.002617, Train ACC:0.9900
Batch 260, Train Loss:0.002607, Train ACC:0.9900
Batch 261, Train Loss:0.002597, Train ACC:0.9900
Batch 262, Train Loss:0.002588, Train ACC:0.9901
Batch 263, Train Loss:0.002578, Train ACC:0.9901
Batch 264, Train Loss:0.002569, Train ACC:0.9902
Batch 265, Train Loss:0.002559, Train ACC:0.9902
Batch 266, Train Loss:0.002550, Train ACC:0.9902
Batch 267, Train Loss:0.002541, Train ACC:0.9903
Batch 268, Train Loss:0.002532, Train ACC:0.9903
Batch 269, Train Loss:0.002523, Train ACC:0.9903
Batch 270, Train Loss:0.002514, Train ACC:0.9904
Batch 271, Train Loss:0.002505, Train ACC:0.9904
Batch 272, Train Loss:0.002496, Train ACC:0.9904
Batch 273, Train Loss:0.002488, Train ACC:0.9905
Batch 274, Train Loss:0.002479, Train ACC:0.9905
Batch 275, Train Loss:0.002470, Train ACC:0.9905
Batch 276, Train Loss:0.002461, Train ACC:0.9906
Batch 277, Train Loss:0.002453, Train ACC:0.9906
Batch 278, Train Loss:0.002445, Train ACC:0.9906
Batch 279, Train Loss:0.002436, Train ACC:0.9907
Batch 280, Train Loss:0.002428, Train ACC:0.9907
Batch 281, Train Loss:0.002419, Train ACC:0.9907
Batch 282, Train Loss:0.002413, Train ACC:0.9908
Batch 283, Train Loss:0.002405, Train ACC:0.9908
Batch 284, Train Loss:0.002397, Train ACC:0.9908
Batch 285, Train Loss:0.002389, Train ACC:0.9909
Batch 286, Train Loss:0.002380, Train ACC:0.9909
Batch 287, Train Loss:0.002372, Train ACC:0.9909
Batch 288, Train Loss:0.002364, Train ACC:0.9910
Batch 289, Train Loss:0.002357, Train ACC:0.9910
Batch 290, Train Loss:0.002349, Train ACC:0.9910
Batch 291, Train Loss:0.002341, Train ACC:0.9911
Batch 292, Train Loss:0.002334, Train ACC:0.9911
Batch 293, Train Loss:0.002327, Train ACC:0.9911
Batch 294, Train Loss:0.002319, Train ACC:0.9912
Batch 295, Train Loss:0.002312, Train ACC:0.9912
Batch 296, Train Loss:0.002304, Train ACC:0.9912
Batch 297, Train Loss:0.002296, Train ACC:0.9912
Batch 298, Train Loss:0.002289, Train ACC:0.9913
Batch 299, Train Loss:0.002281, Train ACC:0.9913
Batch 300, Train Loss:0.002274, Train ACC:0.9913
预测标签：tensor([0, 2, 2, 0, 1, 2, 2, 0, 2, 0, 1, 1, 0, 2, 2, 2, 1, 2, 2, 1, 0, 1, 0, 1,
        2, 0, 0, 1, 2, 0], device='cuda:0'), 真实标签：tensor([0, 2, 2, 0, 1, 2, 2, 0, 2, 0, 1, 1, 0, 2, 2, 2, 1, 2, 2, 1, 0, 1, 0, 1,
        2, 0, 0, 1, 2, 0], device='cuda:0')
MNASNet(
  (layers): Sequential(
    (0): Sequential(
      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    )
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)
    (4): BatchNorm2d(16, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (7): BatchNorm2d(8, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
    (8): Sequential(
      (0): _InvertedResidual(
        (layers): Sequential(
          (0): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(24, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
          (4): BatchNorm2d(24, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(24, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(16, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
        )
      )
      (1): _InvertedResidual(
        (layers): Sequential(
          (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(48, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
          (4): BatchNorm2d(48, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(48, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(16, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
        )
      )
      (2): _InvertedResidual(
        (layers): Sequential(
          (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(48, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
          (4): BatchNorm2d(48, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(48, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(16, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
        )
      )
    )
    (9): Sequential(
      (0): _InvertedResidual(
        (layers): Sequential(
          (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(48, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(48, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=48, bias=False)
          (4): BatchNorm2d(48, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
        )
      )
      (1): _InvertedResidual(
        (layers): Sequential(
          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(72, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)
          (4): BatchNorm2d(72, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
        )
      )
      (2): _InvertedResidual(
        (layers): Sequential(
          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(72, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)
          (4): BatchNorm2d(72, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(24, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
        )
      )
    )
    (10): Sequential(
      (0): _InvertedResidual(
        (layers): Sequential(
          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)
          (4): BatchNorm2d(144, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(40, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
        )
      )
      (1): _InvertedResidual(
        (layers): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
          (4): BatchNorm2d(240, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(40, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
        )
      )
      (2): _InvertedResidual(
        (layers): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
          (4): BatchNorm2d(240, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(40, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
        )
      )
    )
    (11): Sequential(
      (0): _InvertedResidual(
        (layers): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
          (4): BatchNorm2d(240, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(48, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
        )
      )
      (1): _InvertedResidual(
        (layers): Sequential(
          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (4): BatchNorm2d(288, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(48, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
        )
      )
    )
    (12): Sequential(
      (0): _InvertedResidual(
        (layers): Sequential(
          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)
          (4): BatchNorm2d(288, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
        )
      )
      (1): _InvertedResidual(
        (layers): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
        )
      )
      (2): _InvertedResidual(
        (layers): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
        )
      )
      (3): _InvertedResidual(
        (layers): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(96, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
        )
      )
    )
    (13): Sequential(
      (0): _InvertedResidual(
        (layers): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (4): BatchNorm2d(576, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (7): BatchNorm2d(160, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
        )
      )
    )
    (14): Conv2d(160, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (15): BatchNorm2d(1280, eps=1e-05, momentum=0.00029999999999996696, affine=True, track_running_stats=True)
    (16): ReLU(inplace=True)
  )
  (classifier): Sequential(
    (0): Dropout(p=0.2, inplace=True)
    (1): Linear(in_features=1280, out_features=3, bias=True)
  )
)
Batch 1, Train Loss:0.039836, Train ACC:0.2333
Batch 2, Train Loss:0.037683, Train ACC:0.3333
Batch 3, Train Loss:0.036670, Train ACC:0.3778
Batch 4, Train Loss:0.036685, Train ACC:0.3333
Batch 5, Train Loss:0.035534, Train ACC:0.4000
Batch 6, Train Loss:0.034594, Train ACC:0.4444
Batch 7, Train Loss:0.034022, Train ACC:0.4476
Batch 8, Train Loss:0.033585, Train ACC:0.4417
Batch 9, Train Loss:0.033152, Train ACC:0.4481
Batch 10, Train Loss:0.032617, Train ACC:0.4833
Batch 11, Train Loss:0.032012, Train ACC:0.5273
Batch 12, Train Loss:0.031592, Train ACC:0.5472
Batch 13, Train Loss:0.031212, Train ACC:0.5769
Batch 14, Train Loss:0.030659, Train ACC:0.6048
Batch 15, Train Loss:0.030179, Train ACC:0.6311
Batch 16, Train Loss:0.029587, Train ACC:0.6521
Batch 17, Train Loss:0.029445, Train ACC:0.6471
Batch 18, Train Loss:0.028968, Train ACC:0.6593
Batch 19, Train Loss:0.028692, Train ACC:0.6702
Batch 20, Train Loss:0.028273, Train ACC:0.6867
Batch 21, Train Loss:0.027810, Train ACC:0.7016
Batch 22, Train Loss:0.027626, Train ACC:0.7106
Batch 23, Train Loss:0.027182, Train ACC:0.7232
Batch 24, Train Loss:0.026734, Train ACC:0.7347
Batch 25, Train Loss:0.026305, Train ACC:0.7453
Batch 26, Train Loss:0.025878, Train ACC:0.7551
Batch 27, Train Loss:0.025465, Train ACC:0.7642
Batch 28, Train Loss:0.025111, Train ACC:0.7726
Batch 29, Train Loss:0.024666, Train ACC:0.7805
Batch 30, Train Loss:0.024293, Train ACC:0.7878
Batch 31, Train Loss:0.023919, Train ACC:0.7946
Batch 32, Train Loss:0.023694, Train ACC:0.8010
Batch 33, Train Loss:0.023723, Train ACC:0.8040
Batch 34, Train Loss:0.023444, Train ACC:0.8098
Batch 35, Train Loss:0.023138, Train ACC:0.8152
Batch 36, Train Loss:0.022763, Train ACC:0.8204
Batch 37, Train Loss:0.022552, Train ACC:0.8252
Batch 38, Train Loss:0.022306, Train ACC:0.8298
Batch 39, Train Loss:0.022141, Train ACC:0.8342
Batch 40, Train Loss:0.021792, Train ACC:0.8383
Batch 41, Train Loss:0.021588, Train ACC:0.8423
Batch 42, Train Loss:0.021347, Train ACC:0.8460
Batch 43, Train Loss:0.021060, Train ACC:0.8496
Batch 44, Train Loss:0.020746, Train ACC:0.8530
Batch 45, Train Loss:0.020484, Train ACC:0.8563
Batch 46, Train Loss:0.020182, Train ACC:0.8594
Batch 47, Train Loss:0.019911, Train ACC:0.8624
Batch 48, Train Loss:0.019637, Train ACC:0.8653
Batch 49, Train Loss:0.019387, Train ACC:0.8680
Batch 50, Train Loss:0.019106, Train ACC:0.8707
Batch 51, Train Loss:0.018879, Train ACC:0.8732
Batch 52, Train Loss:0.018620, Train ACC:0.8756
Batch 53, Train Loss:0.018394, Train ACC:0.8780
Batch 54, Train Loss:0.018209, Train ACC:0.8802
Batch 55, Train Loss:0.018001, Train ACC:0.8824
Batch 56, Train Loss:0.017787, Train ACC:0.8845
Batch 57, Train Loss:0.017585, Train ACC:0.8865
Batch 58, Train Loss:0.017380, Train ACC:0.8885
Batch 59, Train Loss:0.017175, Train ACC:0.8904
Batch 60, Train Loss:0.016962, Train ACC:0.8922
Batch 61, Train Loss:0.016747, Train ACC:0.8940
Batch 62, Train Loss:0.016551, Train ACC:0.8957
Batch 63, Train Loss:0.016395, Train ACC:0.8974
Batch 64, Train Loss:0.016200, Train ACC:0.8990
Batch 65, Train Loss:0.016017, Train ACC:0.9005
Batch 66, Train Loss:0.015829, Train ACC:0.9020
Batch 67, Train Loss:0.015675, Train ACC:0.9035
Batch 68, Train Loss:0.015491, Train ACC:0.9049
Batch 69, Train Loss:0.015315, Train ACC:0.9063
Batch 70, Train Loss:0.015200, Train ACC:0.9076
Batch 71, Train Loss:0.015090, Train ACC:0.9089
Batch 72, Train Loss:0.014986, Train ACC:0.9102
Batch 73, Train Loss:0.014812, Train ACC:0.9114
Batch 74, Train Loss:0.014667, Train ACC:0.9126
Batch 75, Train Loss:0.014499, Train ACC:0.9138
Batch 76, Train Loss:0.014342, Train ACC:0.9149
Batch 77, Train Loss:0.014224, Train ACC:0.9160
Batch 78, Train Loss:0.014082, Train ACC:0.9171
Batch 79, Train Loss:0.013940, Train ACC:0.9181
Batch 80, Train Loss:0.013792, Train ACC:0.9192
Batch 81, Train Loss:0.013652, Train ACC:0.9202
Batch 82, Train Loss:0.013542, Train ACC:0.9211
Batch 83, Train Loss:0.013502, Train ACC:0.9221
Batch 84, Train Loss:0.013472, Train ACC:0.9230
Batch 85, Train Loss:0.013334, Train ACC:0.9239
Batch 86, Train Loss:0.013205, Train ACC:0.9248
Batch 87, Train Loss:0.013077, Train ACC:0.9257
Batch 88, Train Loss:0.012980, Train ACC:0.9265
Batch 89, Train Loss:0.012884, Train ACC:0.9273
Batch 90, Train Loss:0.012766, Train ACC:0.9281
Batch 91, Train Loss:0.012652, Train ACC:0.9289
Batch 92, Train Loss:0.012546, Train ACC:0.9297
Batch 93, Train Loss:0.012457, Train ACC:0.9305
Batch 94, Train Loss:0.012345, Train ACC:0.9312
Batch 95, Train Loss:0.012275, Train ACC:0.9319
Batch 96, Train Loss:0.012168, Train ACC:0.9326
Batch 97, Train Loss:0.012080, Train ACC:0.9333
Batch 98, Train Loss:0.011999, Train ACC:0.9340
Batch 99, Train Loss:0.011902, Train ACC:0.9347
Batch 100, Train Loss:0.011799, Train ACC:0.9353
Batch 101, Train Loss:0.011711, Train ACC:0.9360
Batch 102, Train Loss:0.011620, Train ACC:0.9366
Batch 103, Train Loss:0.011526, Train ACC:0.9372
Batch 104, Train Loss:0.011432, Train ACC:0.9378
Batch 105, Train Loss:0.011335, Train ACC:0.9384
Batch 106, Train Loss:0.011243, Train ACC:0.9390
Batch 107, Train Loss:0.011150, Train ACC:0.9396
Batch 108, Train Loss:0.011087, Train ACC:0.9401
Batch 109, Train Loss:0.011004, Train ACC:0.9407
Batch 110, Train Loss:0.010932, Train ACC:0.9412
Batch 111, Train Loss:0.010843, Train ACC:0.9417
Batch 112, Train Loss:0.010765, Train ACC:0.9423
Batch 113, Train Loss:0.010680, Train ACC:0.9428
Batch 114, Train Loss:0.010609, Train ACC:0.9433
Batch 115, Train Loss:0.010534, Train ACC:0.9438
Batch 116, Train Loss:0.010453, Train ACC:0.9443
Batch 117, Train Loss:0.010373, Train ACC:0.9447
Batch 118, Train Loss:0.010295, Train ACC:0.9452
Batch 119, Train Loss:0.010219, Train ACC:0.9457
Batch 120, Train Loss:0.010148, Train ACC:0.9461
Batch 121, Train Loss:0.010072, Train ACC:0.9466
Batch 122, Train Loss:0.010003, Train ACC:0.9470
Batch 123, Train Loss:0.009934, Train ACC:0.9474
Batch 124, Train Loss:0.009879, Train ACC:0.9478
Batch 125, Train Loss:0.009811, Train ACC:0.9483
Batch 126, Train Loss:0.009741, Train ACC:0.9487
Batch 127, Train Loss:0.009673, Train ACC:0.9491
Batch 128, Train Loss:0.009604, Train ACC:0.9495
Batch 129, Train Loss:0.009543, Train ACC:0.9499
Batch 130, Train Loss:0.009479, Train ACC:0.9503
Batch 131, Train Loss:0.009415, Train ACC:0.9506
Batch 132, Train Loss:0.009350, Train ACC:0.9510
Batch 133, Train Loss:0.009288, Train ACC:0.9514
Batch 134, Train Loss:0.009230, Train ACC:0.9517
Batch 135, Train Loss:0.009185, Train ACC:0.9521
Batch 136, Train Loss:0.009124, Train ACC:0.9525
Batch 137, Train Loss:0.009063, Train ACC:0.9528
Batch 138, Train Loss:0.009009, Train ACC:0.9531
Batch 139, Train Loss:0.008955, Train ACC:0.9535
Batch 140, Train Loss:0.008907, Train ACC:0.9538
Batch 141, Train Loss:0.008850, Train ACC:0.9541
Batch 142, Train Loss:0.008793, Train ACC:0.9545
Batch 143, Train Loss:0.008735, Train ACC:0.9548
Batch 144, Train Loss:0.008679, Train ACC:0.9551
Batch 145, Train Loss:0.008623, Train ACC:0.9554
Batch 146, Train Loss:0.008569, Train ACC:0.9557
Batch 147, Train Loss:0.008517, Train ACC:0.9560
Batch 148, Train Loss:0.008462, Train ACC:0.9563
Batch 149, Train Loss:0.008413, Train ACC:0.9566
Batch 150, Train Loss:0.008365, Train ACC:0.9569
Batch 151, Train Loss:0.008339, Train ACC:0.9572
Batch 152, Train Loss:0.008295, Train ACC:0.9575
Batch 153, Train Loss:0.008254, Train ACC:0.9577
Batch 154, Train Loss:0.008204, Train ACC:0.9580
Batch 155, Train Loss:0.008155, Train ACC:0.9583
Batch 156, Train Loss:0.008108, Train ACC:0.9585
Batch 157, Train Loss:0.008065, Train ACC:0.9588
Batch 158, Train Loss:0.008021, Train ACC:0.9591
Batch 159, Train Loss:0.007977, Train ACC:0.9593
Batch 160, Train Loss:0.007932, Train ACC:0.9596
Batch 161, Train Loss:0.007887, Train ACC:0.9598
Batch 162, Train Loss:0.007842, Train ACC:0.9601
Batch 163, Train Loss:0.007804, Train ACC:0.9603
Batch 164, Train Loss:0.007764, Train ACC:0.9606
Batch 165, Train Loss:0.007754, Train ACC:0.9608
Batch 166, Train Loss:0.007711, Train ACC:0.9610
Batch 167, Train Loss:0.007671, Train ACC:0.9613
Batch 168, Train Loss:0.007629, Train ACC:0.9615
Batch 169, Train Loss:0.007591, Train ACC:0.9617
Batch 170, Train Loss:0.007550, Train ACC:0.9620
Batch 171, Train Loss:0.007510, Train ACC:0.9622
Batch 172, Train Loss:0.007472, Train ACC:0.9624
Batch 173, Train Loss:0.007440, Train ACC:0.9626
Batch 174, Train Loss:0.007402, Train ACC:0.9628
Batch 175, Train Loss:0.007364, Train ACC:0.9630
Batch 176, Train Loss:0.007326, Train ACC:0.9633
Batch 177, Train Loss:0.007289, Train ACC:0.9635
Batch 178, Train Loss:0.007252, Train ACC:0.9637
Batch 179, Train Loss:0.007215, Train ACC:0.9639
Batch 180, Train Loss:0.007178, Train ACC:0.9641
Batch 181, Train Loss:0.007155, Train ACC:0.9643
Batch 182, Train Loss:0.007123, Train ACC:0.9645
Batch 183, Train Loss:0.007087, Train ACC:0.9647
Batch 184, Train Loss:0.007051, Train ACC:0.9649
Batch 185, Train Loss:0.007019, Train ACC:0.9650
Batch 186, Train Loss:0.006990, Train ACC:0.9652
Batch 187, Train Loss:0.006958, Train ACC:0.9654
Batch 188, Train Loss:0.006924, Train ACC:0.9656
Batch 189, Train Loss:0.006889, Train ACC:0.9658
Batch 190, Train Loss:0.006868, Train ACC:0.9660
Batch 191, Train Loss:0.006834, Train ACC:0.9661
Batch 192, Train Loss:0.006813, Train ACC:0.9663
Batch 193, Train Loss:0.006780, Train ACC:0.9665
Batch 194, Train Loss:0.006764, Train ACC:0.9667
Batch 195, Train Loss:0.006741, Train ACC:0.9668
Batch 196, Train Loss:0.006717, Train ACC:0.9670
Batch 197, Train Loss:0.006686, Train ACC:0.9672
Batch 198, Train Loss:0.006659, Train ACC:0.9673
Batch 199, Train Loss:0.006630, Train ACC:0.9675
Batch 200, Train Loss:0.006601, Train ACC:0.9677
Batch 201, Train Loss:0.006572, Train ACC:0.9678
Batch 202, Train Loss:0.006542, Train ACC:0.9680
Batch 203, Train Loss:0.006513, Train ACC:0.9681
Batch 204, Train Loss:0.006485, Train ACC:0.9683
Batch 205, Train Loss:0.006455, Train ACC:0.9685
Batch 206, Train Loss:0.006427, Train ACC:0.9686
Batch 207, Train Loss:0.006399, Train ACC:0.9688
Batch 208, Train Loss:0.006371, Train ACC:0.9689
Batch 209, Train Loss:0.006342, Train ACC:0.9691
Batch 210, Train Loss:0.006316, Train ACC:0.9692
Batch 211, Train Loss:0.006288, Train ACC:0.9694
Batch 212, Train Loss:0.006263, Train ACC:0.9695
Batch 213, Train Loss:0.006235, Train ACC:0.9696
Batch 214, Train Loss:0.006212, Train ACC:0.9698
Batch 215, Train Loss:0.006186, Train ACC:0.9699
Batch 216, Train Loss:0.006160, Train ACC:0.9701
Batch 217, Train Loss:0.006134, Train ACC:0.9702
Batch 218, Train Loss:0.006107, Train ACC:0.9703
Batch 219, Train Loss:0.006084, Train ACC:0.9705
Batch 220, Train Loss:0.006060, Train ACC:0.9706
Batch 221, Train Loss:0.006043, Train ACC:0.9707
Batch 222, Train Loss:0.006018, Train ACC:0.9709
Batch 223, Train Loss:0.005992, Train ACC:0.9710
Batch 224, Train Loss:0.005969, Train ACC:0.9711
Batch 225, Train Loss:0.005945, Train ACC:0.9713
Batch 226, Train Loss:0.005922, Train ACC:0.9714
Batch 227, Train Loss:0.005898, Train ACC:0.9715
Batch 228, Train Loss:0.005875, Train ACC:0.9716
Batch 229, Train Loss:0.005851, Train ACC:0.9718
Batch 230, Train Loss:0.005827, Train ACC:0.9719
Batch 231, Train Loss:0.005805, Train ACC:0.9720
Batch 232, Train Loss:0.005782, Train ACC:0.9721
Batch 233, Train Loss:0.005763, Train ACC:0.9722
Batch 234, Train Loss:0.005742, Train ACC:0.9724
Batch 235, Train Loss:0.005719, Train ACC:0.9725
Batch 236, Train Loss:0.005698, Train ACC:0.9726
Batch 237, Train Loss:0.005676, Train ACC:0.9727
Batch 238, Train Loss:0.005663, Train ACC:0.9728
Batch 239, Train Loss:0.005641, Train ACC:0.9729
Batch 240, Train Loss:0.005619, Train ACC:0.9731
Batch 241, Train Loss:0.005597, Train ACC:0.9732
Batch 242, Train Loss:0.005577, Train ACC:0.9733
Batch 243, Train Loss:0.005555, Train ACC:0.9734
Batch 244, Train Loss:0.005534, Train ACC:0.9735
Batch 245, Train Loss:0.005514, Train ACC:0.9736
Batch 246, Train Loss:0.005495, Train ACC:0.9737
Batch 247, Train Loss:0.005474, Train ACC:0.9738
Batch 248, Train Loss:0.005454, Train ACC:0.9739
Batch 249, Train Loss:0.005433, Train ACC:0.9740
Batch 250, Train Loss:0.005417, Train ACC:0.9741
Batch 251, Train Loss:0.005398, Train ACC:0.9742
Batch 252, Train Loss:0.005378, Train ACC:0.9743
Batch 253, Train Loss:0.005382, Train ACC:0.9744
Batch 254, Train Loss:0.005362, Train ACC:0.9745
Batch 255, Train Loss:0.005342, Train ACC:0.9746
Batch 256, Train Loss:0.005324, Train ACC:0.9747
Batch 257, Train Loss:0.005305, Train ACC:0.9748
Batch 258, Train Loss:0.005286, Train ACC:0.9749
Batch 259, Train Loss:0.005267, Train ACC:0.9750
Batch 260, Train Loss:0.005250, Train ACC:0.9751
Batch 261, Train Loss:0.005232, Train ACC:0.9752
Batch 262, Train Loss:0.005214, Train ACC:0.9753
Batch 263, Train Loss:0.005196, Train ACC:0.9754
Batch 264, Train Loss:0.005178, Train ACC:0.9755
Batch 265, Train Loss:0.005163, Train ACC:0.9756
Batch 266, Train Loss:0.005146, Train ACC:0.9757
Batch 267, Train Loss:0.005155, Train ACC:0.9758
Batch 268, Train Loss:0.005137, Train ACC:0.9759
Batch 269, Train Loss:0.005120, Train ACC:0.9760
Batch 270, Train Loss:0.005104, Train ACC:0.9760
Batch 271, Train Loss:0.005091, Train ACC:0.9761
Batch 272, Train Loss:0.005073, Train ACC:0.9762
Batch 273, Train Loss:0.005057, Train ACC:0.9763
Batch 274, Train Loss:0.005040, Train ACC:0.9764
Batch 275, Train Loss:0.005026, Train ACC:0.9765
Batch 276, Train Loss:0.005009, Train ACC:0.9766
Batch 277, Train Loss:0.004995, Train ACC:0.9767
Batch 278, Train Loss:0.004980, Train ACC:0.9767
Batch 279, Train Loss:0.004971, Train ACC:0.9768
Batch 280, Train Loss:0.004956, Train ACC:0.9769
Batch 281, Train Loss:0.004939, Train ACC:0.9770
Batch 282, Train Loss:0.004924, Train ACC:0.9771
Batch 283, Train Loss:0.004908, Train ACC:0.9771
Batch 284, Train Loss:0.004892, Train ACC:0.9772
Batch 285, Train Loss:0.004876, Train ACC:0.9773
Batch 286, Train Loss:0.004860, Train ACC:0.9774
Batch 287, Train Loss:0.004848, Train ACC:0.9775
Batch 288, Train Loss:0.004832, Train ACC:0.9775
Batch 289, Train Loss:0.004818, Train ACC:0.9776
Batch 290, Train Loss:0.004804, Train ACC:0.9777
Batch 291, Train Loss:0.004789, Train ACC:0.9778
Batch 292, Train Loss:0.004773, Train ACC:0.9779
Batch 293, Train Loss:0.004758, Train ACC:0.9779
Batch 294, Train Loss:0.004743, Train ACC:0.9780
Batch 295, Train Loss:0.004727, Train ACC:0.9781
Batch 296, Train Loss:0.004714, Train ACC:0.9782
Batch 297, Train Loss:0.004700, Train ACC:0.9782
Batch 298, Train Loss:0.004688, Train ACC:0.9783
Batch 299, Train Loss:0.004673, Train ACC:0.9784
Batch 300, Train Loss:0.004660, Train ACC:0.9784
预测标签：tensor([2, 0, 0, 0, 2, 1, 2, 1, 2, 0, 2, 1, 1, 0, 2, 2, 1, 2, 0, 1, 0, 2, 2, 0,
        2, 2, 2, 2, 2, 0], device='cuda:0'), 真实标签：tensor([2, 0, 0, 0, 2, 1, 2, 1, 2, 0, 2, 1, 1, 0, 2, 2, 1, 2, 0, 1, 0, 2, 2, 0,
        2, 2, 2, 2, 2, 0], device='cuda:0')
VGG(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace=True)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace=True)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=3, bias=True)
  )
)
Batch 1, Train Loss:0.036776, Train ACC:0.1667
Batch 2, Train Loss:0.115942, Train ACC:0.2500
Batch 3, Train Loss:0.184337, Train ACC:0.2333
Batch 4, Train Loss:0.154748, Train ACC:0.3000
Batch 5, Train Loss:0.138850, Train ACC:0.2800
Batch 6, Train Loss:0.121328, Train ACC:0.2944
Batch 7, Train Loss:0.108420, Train ACC:0.3286
Batch 8, Train Loss:0.098742, Train ACC:0.3375
Batch 9, Train Loss:0.092048, Train ACC:0.3333
Batch 10, Train Loss:0.086673, Train ACC:0.3233
Batch 11, Train Loss:0.080973, Train ACC:0.3636
Batch 12, Train Loss:0.076044, Train ACC:0.4167
Batch 13, Train Loss:0.071831, Train ACC:0.4590
Batch 14, Train Loss:0.067889, Train ACC:0.4929
Batch 15, Train Loss:0.064424, Train ACC:0.5222
Batch 16, Train Loss:0.060982, Train ACC:0.5521
Batch 17, Train Loss:0.057844, Train ACC:0.5765
Batch 18, Train Loss:0.055055, Train ACC:0.5981
Batch 19, Train Loss:0.052360, Train ACC:0.6193
Batch 20, Train Loss:0.049827, Train ACC:0.6383
Batch 21, Train Loss:0.047561, Train ACC:0.6556
Batch 22, Train Loss:0.045470, Train ACC:0.6712
Batch 23, Train Loss:0.043518, Train ACC:0.6855
Batch 24, Train Loss:0.041718, Train ACC:0.6986
Batch 25, Train Loss:0.040057, Train ACC:0.7107
Batch 26, Train Loss:0.038529, Train ACC:0.7218
Batch 27, Train Loss:0.037105, Train ACC:0.7321
Batch 28, Train Loss:0.035788, Train ACC:0.7417
Batch 29, Train Loss:0.034554, Train ACC:0.7506
Batch 30, Train Loss:0.033405, Train ACC:0.7589
Batch 31, Train Loss:0.032328, Train ACC:0.7667
Batch 32, Train Loss:0.031319, Train ACC:0.7740
Batch 33, Train Loss:0.030370, Train ACC:0.7808
Batch 34, Train Loss:0.029477, Train ACC:0.7873
Batch 35, Train Loss:0.028635, Train ACC:0.7933
Batch 36, Train Loss:0.027840, Train ACC:0.7991
Batch 37, Train Loss:0.027087, Train ACC:0.8045
Batch 38, Train Loss:0.026375, Train ACC:0.8096
Batch 39, Train Loss:0.025698, Train ACC:0.8145
Batch 40, Train Loss:0.025056, Train ACC:0.8192
Batch 41, Train Loss:0.024445, Train ACC:0.8236
Batch 42, Train Loss:0.023863, Train ACC:0.8278
Batch 43, Train Loss:0.023310, Train ACC:0.8318
Batch 44, Train Loss:0.022780, Train ACC:0.8356
Batch 45, Train Loss:0.022274, Train ACC:0.8393
Batch 46, Train Loss:0.021790, Train ACC:0.8428
Batch 47, Train Loss:0.021326, Train ACC:0.8461
Batch 48, Train Loss:0.020882, Train ACC:0.8493
Batch 49, Train Loss:0.020456, Train ACC:0.8524
Batch 50, Train Loss:0.020047, Train ACC:0.8553
Batch 51, Train Loss:0.019653, Train ACC:0.8582
Batch 52, Train Loss:0.019276, Train ACC:0.8609
Batch 53, Train Loss:0.018912, Train ACC:0.8635
Batch 54, Train Loss:0.018562, Train ACC:0.8660
Batch 55, Train Loss:0.018224, Train ACC:0.8685
Batch 56, Train Loss:0.017899, Train ACC:0.8708
Batch 57, Train Loss:0.017585, Train ACC:0.8731
Batch 58, Train Loss:0.017282, Train ACC:0.8753
Batch 59, Train Loss:0.016989, Train ACC:0.8774
Batch 60, Train Loss:0.016706, Train ACC:0.8794
Batch 61, Train Loss:0.016432, Train ACC:0.8814
Batch 62, Train Loss:0.016167, Train ACC:0.8833
Batch 63, Train Loss:0.015911, Train ACC:0.8852
Batch 64, Train Loss:0.015662, Train ACC:0.8870
Batch 65, Train Loss:0.015421, Train ACC:0.8887
Batch 66, Train Loss:0.015187, Train ACC:0.8904
Batch 67, Train Loss:0.014961, Train ACC:0.8920
Batch 68, Train Loss:0.014741, Train ACC:0.8936
Batch 69, Train Loss:0.014527, Train ACC:0.8952
Batch 70, Train Loss:0.014320, Train ACC:0.8967
Batch 71, Train Loss:0.014118, Train ACC:0.8981
Batch 72, Train Loss:0.013922, Train ACC:0.8995
Batch 73, Train Loss:0.013731, Train ACC:0.9009
Batch 74, Train Loss:0.013546, Train ACC:0.9023
Batch 75, Train Loss:0.013365, Train ACC:0.9036
Batch 76, Train Loss:0.013189, Train ACC:0.9048
Batch 77, Train Loss:0.013018, Train ACC:0.9061
Batch 78, Train Loss:0.012851, Train ACC:0.9073
Batch 79, Train Loss:0.012689, Train ACC:0.9084
Batch 80, Train Loss:0.012530, Train ACC:0.9096
Batch 81, Train Loss:0.012376, Train ACC:0.9107
Batch 82, Train Loss:0.012225, Train ACC:0.9118
Batch 83, Train Loss:0.012078, Train ACC:0.9129
Batch 84, Train Loss:0.011934, Train ACC:0.9139
Batch 85, Train Loss:0.011793, Train ACC:0.9149
Batch 86, Train Loss:0.011656, Train ACC:0.9159
Batch 87, Train Loss:0.011522, Train ACC:0.9169
Batch 88, Train Loss:0.011391, Train ACC:0.9178
Batch 89, Train Loss:0.011263, Train ACC:0.9187
Batch 90, Train Loss:0.011138, Train ACC:0.9196
Batch 91, Train Loss:0.011016, Train ACC:0.9205
Batch 92, Train Loss:0.010896, Train ACC:0.9214
Batch 93, Train Loss:0.010779, Train ACC:0.9222
Batch 94, Train Loss:0.010664, Train ACC:0.9230
Batch 95, Train Loss:0.010552, Train ACC:0.9239
Batch 96, Train Loss:0.010442, Train ACC:0.9247
Batch 97, Train Loss:0.010335, Train ACC:0.9254
Batch 98, Train Loss:0.010229, Train ACC:0.9262
Batch 99, Train Loss:0.010126, Train ACC:0.9269
Batch 100, Train Loss:0.010025, Train ACC:0.9277
Batch 101, Train Loss:0.009925, Train ACC:0.9284
Batch 102, Train Loss:0.009828, Train ACC:0.9291
Batch 103, Train Loss:0.009733, Train ACC:0.9298
Batch 104, Train Loss:0.009639, Train ACC:0.9304
Batch 105, Train Loss:0.009547, Train ACC:0.9311
Batch 106, Train Loss:0.009457, Train ACC:0.9318
Batch 107, Train Loss:0.009369, Train ACC:0.9324
Batch 108, Train Loss:0.009282, Train ACC:0.9330
Batch 109, Train Loss:0.009197, Train ACC:0.9336
Batch 110, Train Loss:0.009113, Train ACC:0.9342
Batch 111, Train Loss:0.009031, Train ACC:0.9348
Batch 112, Train Loss:0.008950, Train ACC:0.9354
Batch 113, Train Loss:0.008871, Train ACC:0.9360
Batch 114, Train Loss:0.008793, Train ACC:0.9365
Batch 115, Train Loss:0.008717, Train ACC:0.9371
Batch 116, Train Loss:0.008642, Train ACC:0.9376
Batch 117, Train Loss:0.008568, Train ACC:0.9382
Batch 118, Train Loss:0.008495, Train ACC:0.9387
Batch 119, Train Loss:0.008424, Train ACC:0.9392
Batch 120, Train Loss:0.008354, Train ACC:0.9397
Batch 121, Train Loss:0.008285, Train ACC:0.9402
Batch 122, Train Loss:0.008217, Train ACC:0.9407
Batch 123, Train Loss:0.008150, Train ACC:0.9412
Batch 124, Train Loss:0.008084, Train ACC:0.9417
Batch 125, Train Loss:0.008020, Train ACC:0.9421
Batch 126, Train Loss:0.007956, Train ACC:0.9426
Batch 127, Train Loss:0.007893, Train ACC:0.9430
Batch 128, Train Loss:0.007832, Train ACC:0.9435
Batch 129, Train Loss:0.007771, Train ACC:0.9439
Batch 130, Train Loss:0.007711, Train ACC:0.9444
Batch 131, Train Loss:0.007652, Train ACC:0.9448
Batch 132, Train Loss:0.007594, Train ACC:0.9452
Batch 133, Train Loss:0.007537, Train ACC:0.9456
Batch 134, Train Loss:0.007481, Train ACC:0.9460
Batch 135, Train Loss:0.007426, Train ACC:0.9464
Batch 136, Train Loss:0.007371, Train ACC:0.9468
Batch 137, Train Loss:0.007317, Train ACC:0.9472
Batch 138, Train Loss:0.007264, Train ACC:0.9476
Batch 139, Train Loss:0.007212, Train ACC:0.9480
Batch 140, Train Loss:0.007161, Train ACC:0.9483
Batch 141, Train Loss:0.007110, Train ACC:0.9487
Batch 142, Train Loss:0.007060, Train ACC:0.9491
Batch 143, Train Loss:0.007010, Train ACC:0.9494
Batch 144, Train Loss:0.006962, Train ACC:0.9498
Batch 145, Train Loss:0.006914, Train ACC:0.9501
Batch 146, Train Loss:0.006866, Train ACC:0.9505
Batch 147, Train Loss:0.006820, Train ACC:0.9508
Batch 148, Train Loss:0.006774, Train ACC:0.9511
Batch 149, Train Loss:0.006728, Train ACC:0.9515
Batch 150, Train Loss:0.006683, Train ACC:0.9518
Batch 151, Train Loss:0.006639, Train ACC:0.9521
Batch 152, Train Loss:0.006595, Train ACC:0.9524
Batch 153, Train Loss:0.006552, Train ACC:0.9527
Batch 154, Train Loss:0.006510, Train ACC:0.9530
Batch 155, Train Loss:0.006468, Train ACC:0.9533
Batch 156, Train Loss:0.006426, Train ACC:0.9536
Batch 157, Train Loss:0.006385, Train ACC:0.9539
Batch 158, Train Loss:0.006345, Train ACC:0.9542
Batch 159, Train Loss:0.006305, Train ACC:0.9545
Batch 160, Train Loss:0.006266, Train ACC:0.9548
Batch 161, Train Loss:0.006227, Train ACC:0.9551
Batch 162, Train Loss:0.006188, Train ACC:0.9553
Batch 163, Train Loss:0.006150, Train ACC:0.9556
Batch 164, Train Loss:0.006113, Train ACC:0.9559
Batch 165, Train Loss:0.006076, Train ACC:0.9562
Batch 166, Train Loss:0.006039, Train ACC:0.9564
Batch 167, Train Loss:0.006003, Train ACC:0.9567
Batch 168, Train Loss:0.005967, Train ACC:0.9569
Batch 169, Train Loss:0.005932, Train ACC:0.9572
Batch 170, Train Loss:0.005897, Train ACC:0.9575
Batch 171, Train Loss:0.005862, Train ACC:0.9577
Batch 172, Train Loss:0.005828, Train ACC:0.9579
Batch 173, Train Loss:0.005795, Train ACC:0.9582
Batch 174, Train Loss:0.005761, Train ACC:0.9584
Batch 175, Train Loss:0.005728, Train ACC:0.9587
Batch 176, Train Loss:0.005696, Train ACC:0.9589
Batch 177, Train Loss:0.005664, Train ACC:0.9591
Batch 178, Train Loss:0.005632, Train ACC:0.9594
Batch 179, Train Loss:0.005600, Train ACC:0.9596
Batch 180, Train Loss:0.005569, Train ACC:0.9598
Batch 181, Train Loss:0.005539, Train ACC:0.9600
Batch 182, Train Loss:0.005508, Train ACC:0.9603
Batch 183, Train Loss:0.005478, Train ACC:0.9605
Batch 184, Train Loss:0.005448, Train ACC:0.9607
Batch 185, Train Loss:0.005419, Train ACC:0.9609
Batch 186, Train Loss:0.005390, Train ACC:0.9611
Batch 187, Train Loss:0.005361, Train ACC:0.9613
Batch 188, Train Loss:0.005332, Train ACC:0.9615
Batch 189, Train Loss:0.005304, Train ACC:0.9617
Batch 190, Train Loss:0.005276, Train ACC:0.9619
Batch 191, Train Loss:0.005249, Train ACC:0.9621
Batch 192, Train Loss:0.005221, Train ACC:0.9623
Batch 193, Train Loss:0.005194, Train ACC:0.9625
Batch 194, Train Loss:0.005167, Train ACC:0.9627
Batch 195, Train Loss:0.005141, Train ACC:0.9629
Batch 196, Train Loss:0.005115, Train ACC:0.9631
Batch 197, Train Loss:0.005089, Train ACC:0.9633
Batch 198, Train Loss:0.005063, Train ACC:0.9635
Batch 199, Train Loss:0.005038, Train ACC:0.9637
Batch 200, Train Loss:0.005012, Train ACC:0.9638
Batch 201, Train Loss:0.004987, Train ACC:0.9640
Batch 202, Train Loss:0.004963, Train ACC:0.9642
Batch 203, Train Loss:0.004938, Train ACC:0.9644
Batch 204, Train Loss:0.004914, Train ACC:0.9645
Batch 205, Train Loss:0.004890, Train ACC:0.9647
Batch 206, Train Loss:0.004866, Train ACC:0.9649
Batch 207, Train Loss:0.004843, Train ACC:0.9651
Batch 208, Train Loss:0.004820, Train ACC:0.9652
Batch 209, Train Loss:0.004797, Train ACC:0.9654
Batch 210, Train Loss:0.004774, Train ACC:0.9656
Batch 211, Train Loss:0.004751, Train ACC:0.9657
Batch 212, Train Loss:0.004729, Train ACC:0.9659
Batch 213, Train Loss:0.004707, Train ACC:0.9660
Batch 214, Train Loss:0.004685, Train ACC:0.9662
Batch 215, Train Loss:0.004663, Train ACC:0.9664
Batch 216, Train Loss:0.004641, Train ACC:0.9665
Batch 217, Train Loss:0.004620, Train ACC:0.9667
Batch 218, Train Loss:0.004599, Train ACC:0.9668
Batch 219, Train Loss:0.004578, Train ACC:0.9670
Batch 220, Train Loss:0.004557, Train ACC:0.9671
Batch 221, Train Loss:0.004536, Train ACC:0.9673
Batch 222, Train Loss:0.004516, Train ACC:0.9674
Batch 223, Train Loss:0.004495, Train ACC:0.9676
Batch 224, Train Loss:0.004475, Train ACC:0.9677
Batch 225, Train Loss:0.004456, Train ACC:0.9679
Batch 226, Train Loss:0.004436, Train ACC:0.9680
Batch 227, Train Loss:0.004416, Train ACC:0.9681
Batch 228, Train Loss:0.004397, Train ACC:0.9683
Batch 229, Train Loss:0.004378, Train ACC:0.9684
Batch 230, Train Loss:0.004359, Train ACC:0.9686
Batch 231, Train Loss:0.004340, Train ACC:0.9687
Batch 232, Train Loss:0.004321, Train ACC:0.9688
Batch 233, Train Loss:0.004303, Train ACC:0.9690
Batch 234, Train Loss:0.004284, Train ACC:0.9691
Batch 235, Train Loss:0.004266, Train ACC:0.9692
Batch 236, Train Loss:0.004248, Train ACC:0.9694
Batch 237, Train Loss:0.004230, Train ACC:0.9695
Batch 238, Train Loss:0.004212, Train ACC:0.9696
Batch 239, Train Loss:0.004195, Train ACC:0.9697
Batch 240, Train Loss:0.004177, Train ACC:0.9699
Batch 241, Train Loss:0.004160, Train ACC:0.9700
Batch 242, Train Loss:0.004143, Train ACC:0.9701
Batch 243, Train Loss:0.004126, Train ACC:0.9702
Batch 244, Train Loss:0.004109, Train ACC:0.9704
Batch 245, Train Loss:0.004092, Train ACC:0.9705
Batch 246, Train Loss:0.004075, Train ACC:0.9706
Batch 247, Train Loss:0.004059, Train ACC:0.9707
Batch 248, Train Loss:0.004042, Train ACC:0.9708
Batch 249, Train Loss:0.004026, Train ACC:0.9710
Batch 250, Train Loss:0.004010, Train ACC:0.9711
Batch 251, Train Loss:0.003994, Train ACC:0.9712
Batch 252, Train Loss:0.003978, Train ACC:0.9713
Batch 253, Train Loss:0.003962, Train ACC:0.9714
Batch 254, Train Loss:0.003947, Train ACC:0.9715
Batch 255, Train Loss:0.003931, Train ACC:0.9716
Batch 256, Train Loss:0.003916, Train ACC:0.9717
Batch 257, Train Loss:0.003901, Train ACC:0.9719
Batch 258, Train Loss:0.003886, Train ACC:0.9720
Batch 259, Train Loss:0.003871, Train ACC:0.9721
Batch 260, Train Loss:0.003856, Train ACC:0.9722
Batch 261, Train Loss:0.003841, Train ACC:0.9723
Batch 262, Train Loss:0.003826, Train ACC:0.9724
Batch 263, Train Loss:0.003812, Train ACC:0.9725
Batch 264, Train Loss:0.003797, Train ACC:0.9726
Batch 265, Train Loss:0.003783, Train ACC:0.9727
Batch 266, Train Loss:0.003769, Train ACC:0.9728
Batch 267, Train Loss:0.003755, Train ACC:0.9729
Batch 268, Train Loss:0.003741, Train ACC:0.9730
Batch 269, Train Loss:0.003727, Train ACC:0.9731
Batch 270, Train Loss:0.003713, Train ACC:0.9732
Batch 271, Train Loss:0.003699, Train ACC:0.9733
Batch 272, Train Loss:0.003686, Train ACC:0.9734
Batch 273, Train Loss:0.003672, Train ACC:0.9735
Batch 274, Train Loss:0.003659, Train ACC:0.9736
Batch 275, Train Loss:0.003645, Train ACC:0.9737
Batch 276, Train Loss:0.003632, Train ACC:0.9738
Batch 277, Train Loss:0.003619, Train ACC:0.9739
Batch 278, Train Loss:0.003606, Train ACC:0.9740
Batch 279, Train Loss:0.003593, Train ACC:0.9741
Batch 280, Train Loss:0.003580, Train ACC:0.9742
Batch 281, Train Loss:0.003568, Train ACC:0.9743
Batch 282, Train Loss:0.003555, Train ACC:0.9743
Batch 283, Train Loss:0.003542, Train ACC:0.9744
Batch 284, Train Loss:0.003530, Train ACC:0.9745
Batch 285, Train Loss:0.003518, Train ACC:0.9746
Batch 286, Train Loss:0.003505, Train ACC:0.9747
Batch 287, Train Loss:0.003493, Train ACC:0.9748
Batch 288, Train Loss:0.003481, Train ACC:0.9749
Batch 289, Train Loss:0.003469, Train ACC:0.9750
Batch 290, Train Loss:0.003457, Train ACC:0.9751
Batch 291, Train Loss:0.003445, Train ACC:0.9751
Batch 292, Train Loss:0.003433, Train ACC:0.9752
Batch 293, Train Loss:0.003422, Train ACC:0.9753
Batch 294, Train Loss:0.003410, Train ACC:0.9754
Batch 295, Train Loss:0.003398, Train ACC:0.9755
Batch 296, Train Loss:0.003387, Train ACC:0.9756
Batch 297, Train Loss:0.003375, Train ACC:0.9756
Batch 298, Train Loss:0.003364, Train ACC:0.9757
Batch 299, Train Loss:0.003353, Train ACC:0.9758
Batch 300, Train Loss:0.003342, Train ACC:0.9759
预测标签：tensor([2, 2, 0, 2, 2, 2, 0, 0, 2, 0, 2, 2, 0, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2,
        0, 0, 1, 1, 1, 2], device='cuda:0'), 真实标签：tensor([2, 2, 0, 2, 2, 2, 0, 0, 2, 0, 2, 2, 0, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2,
        0, 0, 1, 1, 1, 2], device='cuda:0')
